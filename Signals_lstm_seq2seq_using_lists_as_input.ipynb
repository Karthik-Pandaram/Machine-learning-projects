{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Signals lstm_seq2seq_using_lists as input",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthik-Pandaram/Machine-learning-projects/blob/main/Signals_lstm_seq2seq_using_lists_as_input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0G4eeSuCfUA"
      },
      "source": [
        "# Character-level recurrent sequence-to-sequence model\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2017/09/29<br>\n",
        "**Last modified:** 2020/04/26<br>\n",
        "**Description:** Character-level recurrent sequence-to-sequence model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9uzSJhvCfUH"
      },
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9roA2p4gCfUI"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87_iLTniCfUJ"
      },
      "source": [
        "## Download the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isc5hQVeCfUL"
      },
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06jsjJBQCfUM"
      },
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwuY5eT4CfUN"
      },
      "source": [
        "## Prepare the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_k22rQzlw9k"
      },
      "source": [
        "#Change input text to sequences \n",
        "X = list()\n",
        "Y = list()\n",
        "X = [x for x in range(0, 1000, 1)]\n",
        "Y = [y for y in range(1000, 0, -1)]\n",
        "\n",
        "X = np.array(X).reshape(20, 50)\n",
        "Y = np.array(Y).reshape(20, 50)\n",
        "inwave = [ ]\n",
        "tarwave = [ ]\n",
        "for wav in X:\n",
        " inwave.append(wav.tolist())\n",
        "for wav in Y:\n",
        " tarwave.append(wav.tolist())"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcKNVZpiDvtW",
        "outputId": "f96fe5da-001a-4e54-d7c8-566a67f29b0d"
      },
      "source": [
        "tarwave"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1000,\n",
              "  999,\n",
              "  998,\n",
              "  997,\n",
              "  996,\n",
              "  995,\n",
              "  994,\n",
              "  993,\n",
              "  992,\n",
              "  991,\n",
              "  990,\n",
              "  989,\n",
              "  988,\n",
              "  987,\n",
              "  986,\n",
              "  985,\n",
              "  984,\n",
              "  983,\n",
              "  982,\n",
              "  981,\n",
              "  980,\n",
              "  979,\n",
              "  978,\n",
              "  977,\n",
              "  976,\n",
              "  975,\n",
              "  974,\n",
              "  973,\n",
              "  972,\n",
              "  971,\n",
              "  970,\n",
              "  969,\n",
              "  968,\n",
              "  967,\n",
              "  966,\n",
              "  965,\n",
              "  964,\n",
              "  963,\n",
              "  962,\n",
              "  961,\n",
              "  960,\n",
              "  959,\n",
              "  958,\n",
              "  957,\n",
              "  956,\n",
              "  955,\n",
              "  954,\n",
              "  953,\n",
              "  952,\n",
              "  951],\n",
              " [950,\n",
              "  949,\n",
              "  948,\n",
              "  947,\n",
              "  946,\n",
              "  945,\n",
              "  944,\n",
              "  943,\n",
              "  942,\n",
              "  941,\n",
              "  940,\n",
              "  939,\n",
              "  938,\n",
              "  937,\n",
              "  936,\n",
              "  935,\n",
              "  934,\n",
              "  933,\n",
              "  932,\n",
              "  931,\n",
              "  930,\n",
              "  929,\n",
              "  928,\n",
              "  927,\n",
              "  926,\n",
              "  925,\n",
              "  924,\n",
              "  923,\n",
              "  922,\n",
              "  921,\n",
              "  920,\n",
              "  919,\n",
              "  918,\n",
              "  917,\n",
              "  916,\n",
              "  915,\n",
              "  914,\n",
              "  913,\n",
              "  912,\n",
              "  911,\n",
              "  910,\n",
              "  909,\n",
              "  908,\n",
              "  907,\n",
              "  906,\n",
              "  905,\n",
              "  904,\n",
              "  903,\n",
              "  902,\n",
              "  901],\n",
              " [900,\n",
              "  899,\n",
              "  898,\n",
              "  897,\n",
              "  896,\n",
              "  895,\n",
              "  894,\n",
              "  893,\n",
              "  892,\n",
              "  891,\n",
              "  890,\n",
              "  889,\n",
              "  888,\n",
              "  887,\n",
              "  886,\n",
              "  885,\n",
              "  884,\n",
              "  883,\n",
              "  882,\n",
              "  881,\n",
              "  880,\n",
              "  879,\n",
              "  878,\n",
              "  877,\n",
              "  876,\n",
              "  875,\n",
              "  874,\n",
              "  873,\n",
              "  872,\n",
              "  871,\n",
              "  870,\n",
              "  869,\n",
              "  868,\n",
              "  867,\n",
              "  866,\n",
              "  865,\n",
              "  864,\n",
              "  863,\n",
              "  862,\n",
              "  861,\n",
              "  860,\n",
              "  859,\n",
              "  858,\n",
              "  857,\n",
              "  856,\n",
              "  855,\n",
              "  854,\n",
              "  853,\n",
              "  852,\n",
              "  851],\n",
              " [850,\n",
              "  849,\n",
              "  848,\n",
              "  847,\n",
              "  846,\n",
              "  845,\n",
              "  844,\n",
              "  843,\n",
              "  842,\n",
              "  841,\n",
              "  840,\n",
              "  839,\n",
              "  838,\n",
              "  837,\n",
              "  836,\n",
              "  835,\n",
              "  834,\n",
              "  833,\n",
              "  832,\n",
              "  831,\n",
              "  830,\n",
              "  829,\n",
              "  828,\n",
              "  827,\n",
              "  826,\n",
              "  825,\n",
              "  824,\n",
              "  823,\n",
              "  822,\n",
              "  821,\n",
              "  820,\n",
              "  819,\n",
              "  818,\n",
              "  817,\n",
              "  816,\n",
              "  815,\n",
              "  814,\n",
              "  813,\n",
              "  812,\n",
              "  811,\n",
              "  810,\n",
              "  809,\n",
              "  808,\n",
              "  807,\n",
              "  806,\n",
              "  805,\n",
              "  804,\n",
              "  803,\n",
              "  802,\n",
              "  801],\n",
              " [800,\n",
              "  799,\n",
              "  798,\n",
              "  797,\n",
              "  796,\n",
              "  795,\n",
              "  794,\n",
              "  793,\n",
              "  792,\n",
              "  791,\n",
              "  790,\n",
              "  789,\n",
              "  788,\n",
              "  787,\n",
              "  786,\n",
              "  785,\n",
              "  784,\n",
              "  783,\n",
              "  782,\n",
              "  781,\n",
              "  780,\n",
              "  779,\n",
              "  778,\n",
              "  777,\n",
              "  776,\n",
              "  775,\n",
              "  774,\n",
              "  773,\n",
              "  772,\n",
              "  771,\n",
              "  770,\n",
              "  769,\n",
              "  768,\n",
              "  767,\n",
              "  766,\n",
              "  765,\n",
              "  764,\n",
              "  763,\n",
              "  762,\n",
              "  761,\n",
              "  760,\n",
              "  759,\n",
              "  758,\n",
              "  757,\n",
              "  756,\n",
              "  755,\n",
              "  754,\n",
              "  753,\n",
              "  752,\n",
              "  751],\n",
              " [750,\n",
              "  749,\n",
              "  748,\n",
              "  747,\n",
              "  746,\n",
              "  745,\n",
              "  744,\n",
              "  743,\n",
              "  742,\n",
              "  741,\n",
              "  740,\n",
              "  739,\n",
              "  738,\n",
              "  737,\n",
              "  736,\n",
              "  735,\n",
              "  734,\n",
              "  733,\n",
              "  732,\n",
              "  731,\n",
              "  730,\n",
              "  729,\n",
              "  728,\n",
              "  727,\n",
              "  726,\n",
              "  725,\n",
              "  724,\n",
              "  723,\n",
              "  722,\n",
              "  721,\n",
              "  720,\n",
              "  719,\n",
              "  718,\n",
              "  717,\n",
              "  716,\n",
              "  715,\n",
              "  714,\n",
              "  713,\n",
              "  712,\n",
              "  711,\n",
              "  710,\n",
              "  709,\n",
              "  708,\n",
              "  707,\n",
              "  706,\n",
              "  705,\n",
              "  704,\n",
              "  703,\n",
              "  702,\n",
              "  701],\n",
              " [700,\n",
              "  699,\n",
              "  698,\n",
              "  697,\n",
              "  696,\n",
              "  695,\n",
              "  694,\n",
              "  693,\n",
              "  692,\n",
              "  691,\n",
              "  690,\n",
              "  689,\n",
              "  688,\n",
              "  687,\n",
              "  686,\n",
              "  685,\n",
              "  684,\n",
              "  683,\n",
              "  682,\n",
              "  681,\n",
              "  680,\n",
              "  679,\n",
              "  678,\n",
              "  677,\n",
              "  676,\n",
              "  675,\n",
              "  674,\n",
              "  673,\n",
              "  672,\n",
              "  671,\n",
              "  670,\n",
              "  669,\n",
              "  668,\n",
              "  667,\n",
              "  666,\n",
              "  665,\n",
              "  664,\n",
              "  663,\n",
              "  662,\n",
              "  661,\n",
              "  660,\n",
              "  659,\n",
              "  658,\n",
              "  657,\n",
              "  656,\n",
              "  655,\n",
              "  654,\n",
              "  653,\n",
              "  652,\n",
              "  651],\n",
              " [650,\n",
              "  649,\n",
              "  648,\n",
              "  647,\n",
              "  646,\n",
              "  645,\n",
              "  644,\n",
              "  643,\n",
              "  642,\n",
              "  641,\n",
              "  640,\n",
              "  639,\n",
              "  638,\n",
              "  637,\n",
              "  636,\n",
              "  635,\n",
              "  634,\n",
              "  633,\n",
              "  632,\n",
              "  631,\n",
              "  630,\n",
              "  629,\n",
              "  628,\n",
              "  627,\n",
              "  626,\n",
              "  625,\n",
              "  624,\n",
              "  623,\n",
              "  622,\n",
              "  621,\n",
              "  620,\n",
              "  619,\n",
              "  618,\n",
              "  617,\n",
              "  616,\n",
              "  615,\n",
              "  614,\n",
              "  613,\n",
              "  612,\n",
              "  611,\n",
              "  610,\n",
              "  609,\n",
              "  608,\n",
              "  607,\n",
              "  606,\n",
              "  605,\n",
              "  604,\n",
              "  603,\n",
              "  602,\n",
              "  601],\n",
              " [600,\n",
              "  599,\n",
              "  598,\n",
              "  597,\n",
              "  596,\n",
              "  595,\n",
              "  594,\n",
              "  593,\n",
              "  592,\n",
              "  591,\n",
              "  590,\n",
              "  589,\n",
              "  588,\n",
              "  587,\n",
              "  586,\n",
              "  585,\n",
              "  584,\n",
              "  583,\n",
              "  582,\n",
              "  581,\n",
              "  580,\n",
              "  579,\n",
              "  578,\n",
              "  577,\n",
              "  576,\n",
              "  575,\n",
              "  574,\n",
              "  573,\n",
              "  572,\n",
              "  571,\n",
              "  570,\n",
              "  569,\n",
              "  568,\n",
              "  567,\n",
              "  566,\n",
              "  565,\n",
              "  564,\n",
              "  563,\n",
              "  562,\n",
              "  561,\n",
              "  560,\n",
              "  559,\n",
              "  558,\n",
              "  557,\n",
              "  556,\n",
              "  555,\n",
              "  554,\n",
              "  553,\n",
              "  552,\n",
              "  551],\n",
              " [550,\n",
              "  549,\n",
              "  548,\n",
              "  547,\n",
              "  546,\n",
              "  545,\n",
              "  544,\n",
              "  543,\n",
              "  542,\n",
              "  541,\n",
              "  540,\n",
              "  539,\n",
              "  538,\n",
              "  537,\n",
              "  536,\n",
              "  535,\n",
              "  534,\n",
              "  533,\n",
              "  532,\n",
              "  531,\n",
              "  530,\n",
              "  529,\n",
              "  528,\n",
              "  527,\n",
              "  526,\n",
              "  525,\n",
              "  524,\n",
              "  523,\n",
              "  522,\n",
              "  521,\n",
              "  520,\n",
              "  519,\n",
              "  518,\n",
              "  517,\n",
              "  516,\n",
              "  515,\n",
              "  514,\n",
              "  513,\n",
              "  512,\n",
              "  511,\n",
              "  510,\n",
              "  509,\n",
              "  508,\n",
              "  507,\n",
              "  506,\n",
              "  505,\n",
              "  504,\n",
              "  503,\n",
              "  502,\n",
              "  501],\n",
              " [500,\n",
              "  499,\n",
              "  498,\n",
              "  497,\n",
              "  496,\n",
              "  495,\n",
              "  494,\n",
              "  493,\n",
              "  492,\n",
              "  491,\n",
              "  490,\n",
              "  489,\n",
              "  488,\n",
              "  487,\n",
              "  486,\n",
              "  485,\n",
              "  484,\n",
              "  483,\n",
              "  482,\n",
              "  481,\n",
              "  480,\n",
              "  479,\n",
              "  478,\n",
              "  477,\n",
              "  476,\n",
              "  475,\n",
              "  474,\n",
              "  473,\n",
              "  472,\n",
              "  471,\n",
              "  470,\n",
              "  469,\n",
              "  468,\n",
              "  467,\n",
              "  466,\n",
              "  465,\n",
              "  464,\n",
              "  463,\n",
              "  462,\n",
              "  461,\n",
              "  460,\n",
              "  459,\n",
              "  458,\n",
              "  457,\n",
              "  456,\n",
              "  455,\n",
              "  454,\n",
              "  453,\n",
              "  452,\n",
              "  451],\n",
              " [450,\n",
              "  449,\n",
              "  448,\n",
              "  447,\n",
              "  446,\n",
              "  445,\n",
              "  444,\n",
              "  443,\n",
              "  442,\n",
              "  441,\n",
              "  440,\n",
              "  439,\n",
              "  438,\n",
              "  437,\n",
              "  436,\n",
              "  435,\n",
              "  434,\n",
              "  433,\n",
              "  432,\n",
              "  431,\n",
              "  430,\n",
              "  429,\n",
              "  428,\n",
              "  427,\n",
              "  426,\n",
              "  425,\n",
              "  424,\n",
              "  423,\n",
              "  422,\n",
              "  421,\n",
              "  420,\n",
              "  419,\n",
              "  418,\n",
              "  417,\n",
              "  416,\n",
              "  415,\n",
              "  414,\n",
              "  413,\n",
              "  412,\n",
              "  411,\n",
              "  410,\n",
              "  409,\n",
              "  408,\n",
              "  407,\n",
              "  406,\n",
              "  405,\n",
              "  404,\n",
              "  403,\n",
              "  402,\n",
              "  401],\n",
              " [400,\n",
              "  399,\n",
              "  398,\n",
              "  397,\n",
              "  396,\n",
              "  395,\n",
              "  394,\n",
              "  393,\n",
              "  392,\n",
              "  391,\n",
              "  390,\n",
              "  389,\n",
              "  388,\n",
              "  387,\n",
              "  386,\n",
              "  385,\n",
              "  384,\n",
              "  383,\n",
              "  382,\n",
              "  381,\n",
              "  380,\n",
              "  379,\n",
              "  378,\n",
              "  377,\n",
              "  376,\n",
              "  375,\n",
              "  374,\n",
              "  373,\n",
              "  372,\n",
              "  371,\n",
              "  370,\n",
              "  369,\n",
              "  368,\n",
              "  367,\n",
              "  366,\n",
              "  365,\n",
              "  364,\n",
              "  363,\n",
              "  362,\n",
              "  361,\n",
              "  360,\n",
              "  359,\n",
              "  358,\n",
              "  357,\n",
              "  356,\n",
              "  355,\n",
              "  354,\n",
              "  353,\n",
              "  352,\n",
              "  351],\n",
              " [350,\n",
              "  349,\n",
              "  348,\n",
              "  347,\n",
              "  346,\n",
              "  345,\n",
              "  344,\n",
              "  343,\n",
              "  342,\n",
              "  341,\n",
              "  340,\n",
              "  339,\n",
              "  338,\n",
              "  337,\n",
              "  336,\n",
              "  335,\n",
              "  334,\n",
              "  333,\n",
              "  332,\n",
              "  331,\n",
              "  330,\n",
              "  329,\n",
              "  328,\n",
              "  327,\n",
              "  326,\n",
              "  325,\n",
              "  324,\n",
              "  323,\n",
              "  322,\n",
              "  321,\n",
              "  320,\n",
              "  319,\n",
              "  318,\n",
              "  317,\n",
              "  316,\n",
              "  315,\n",
              "  314,\n",
              "  313,\n",
              "  312,\n",
              "  311,\n",
              "  310,\n",
              "  309,\n",
              "  308,\n",
              "  307,\n",
              "  306,\n",
              "  305,\n",
              "  304,\n",
              "  303,\n",
              "  302,\n",
              "  301],\n",
              " [300,\n",
              "  299,\n",
              "  298,\n",
              "  297,\n",
              "  296,\n",
              "  295,\n",
              "  294,\n",
              "  293,\n",
              "  292,\n",
              "  291,\n",
              "  290,\n",
              "  289,\n",
              "  288,\n",
              "  287,\n",
              "  286,\n",
              "  285,\n",
              "  284,\n",
              "  283,\n",
              "  282,\n",
              "  281,\n",
              "  280,\n",
              "  279,\n",
              "  278,\n",
              "  277,\n",
              "  276,\n",
              "  275,\n",
              "  274,\n",
              "  273,\n",
              "  272,\n",
              "  271,\n",
              "  270,\n",
              "  269,\n",
              "  268,\n",
              "  267,\n",
              "  266,\n",
              "  265,\n",
              "  264,\n",
              "  263,\n",
              "  262,\n",
              "  261,\n",
              "  260,\n",
              "  259,\n",
              "  258,\n",
              "  257,\n",
              "  256,\n",
              "  255,\n",
              "  254,\n",
              "  253,\n",
              "  252,\n",
              "  251],\n",
              " [250,\n",
              "  249,\n",
              "  248,\n",
              "  247,\n",
              "  246,\n",
              "  245,\n",
              "  244,\n",
              "  243,\n",
              "  242,\n",
              "  241,\n",
              "  240,\n",
              "  239,\n",
              "  238,\n",
              "  237,\n",
              "  236,\n",
              "  235,\n",
              "  234,\n",
              "  233,\n",
              "  232,\n",
              "  231,\n",
              "  230,\n",
              "  229,\n",
              "  228,\n",
              "  227,\n",
              "  226,\n",
              "  225,\n",
              "  224,\n",
              "  223,\n",
              "  222,\n",
              "  221,\n",
              "  220,\n",
              "  219,\n",
              "  218,\n",
              "  217,\n",
              "  216,\n",
              "  215,\n",
              "  214,\n",
              "  213,\n",
              "  212,\n",
              "  211,\n",
              "  210,\n",
              "  209,\n",
              "  208,\n",
              "  207,\n",
              "  206,\n",
              "  205,\n",
              "  204,\n",
              "  203,\n",
              "  202,\n",
              "  201],\n",
              " [200,\n",
              "  199,\n",
              "  198,\n",
              "  197,\n",
              "  196,\n",
              "  195,\n",
              "  194,\n",
              "  193,\n",
              "  192,\n",
              "  191,\n",
              "  190,\n",
              "  189,\n",
              "  188,\n",
              "  187,\n",
              "  186,\n",
              "  185,\n",
              "  184,\n",
              "  183,\n",
              "  182,\n",
              "  181,\n",
              "  180,\n",
              "  179,\n",
              "  178,\n",
              "  177,\n",
              "  176,\n",
              "  175,\n",
              "  174,\n",
              "  173,\n",
              "  172,\n",
              "  171,\n",
              "  170,\n",
              "  169,\n",
              "  168,\n",
              "  167,\n",
              "  166,\n",
              "  165,\n",
              "  164,\n",
              "  163,\n",
              "  162,\n",
              "  161,\n",
              "  160,\n",
              "  159,\n",
              "  158,\n",
              "  157,\n",
              "  156,\n",
              "  155,\n",
              "  154,\n",
              "  153,\n",
              "  152,\n",
              "  151],\n",
              " [150,\n",
              "  149,\n",
              "  148,\n",
              "  147,\n",
              "  146,\n",
              "  145,\n",
              "  144,\n",
              "  143,\n",
              "  142,\n",
              "  141,\n",
              "  140,\n",
              "  139,\n",
              "  138,\n",
              "  137,\n",
              "  136,\n",
              "  135,\n",
              "  134,\n",
              "  133,\n",
              "  132,\n",
              "  131,\n",
              "  130,\n",
              "  129,\n",
              "  128,\n",
              "  127,\n",
              "  126,\n",
              "  125,\n",
              "  124,\n",
              "  123,\n",
              "  122,\n",
              "  121,\n",
              "  120,\n",
              "  119,\n",
              "  118,\n",
              "  117,\n",
              "  116,\n",
              "  115,\n",
              "  114,\n",
              "  113,\n",
              "  112,\n",
              "  111,\n",
              "  110,\n",
              "  109,\n",
              "  108,\n",
              "  107,\n",
              "  106,\n",
              "  105,\n",
              "  104,\n",
              "  103,\n",
              "  102,\n",
              "  101],\n",
              " [100,\n",
              "  99,\n",
              "  98,\n",
              "  97,\n",
              "  96,\n",
              "  95,\n",
              "  94,\n",
              "  93,\n",
              "  92,\n",
              "  91,\n",
              "  90,\n",
              "  89,\n",
              "  88,\n",
              "  87,\n",
              "  86,\n",
              "  85,\n",
              "  84,\n",
              "  83,\n",
              "  82,\n",
              "  81,\n",
              "  80,\n",
              "  79,\n",
              "  78,\n",
              "  77,\n",
              "  76,\n",
              "  75,\n",
              "  74,\n",
              "  73,\n",
              "  72,\n",
              "  71,\n",
              "  70,\n",
              "  69,\n",
              "  68,\n",
              "  67,\n",
              "  66,\n",
              "  65,\n",
              "  64,\n",
              "  63,\n",
              "  62,\n",
              "  61,\n",
              "  60,\n",
              "  59,\n",
              "  58,\n",
              "  57,\n",
              "  56,\n",
              "  55,\n",
              "  54,\n",
              "  53,\n",
              "  52,\n",
              "  51],\n",
              " [50,\n",
              "  49,\n",
              "  48,\n",
              "  47,\n",
              "  46,\n",
              "  45,\n",
              "  44,\n",
              "  43,\n",
              "  42,\n",
              "  41,\n",
              "  40,\n",
              "  39,\n",
              "  38,\n",
              "  37,\n",
              "  36,\n",
              "  35,\n",
              "  34,\n",
              "  33,\n",
              "  32,\n",
              "  31,\n",
              "  30,\n",
              "  29,\n",
              "  28,\n",
              "  27,\n",
              "  26,\n",
              "  25,\n",
              "  24,\n",
              "  23,\n",
              "  22,\n",
              "  21,\n",
              "  20,\n",
              "  19,\n",
              "  18,\n",
              "  17,\n",
              "  16,\n",
              "  15,\n",
              "  14,\n",
              "  13,\n",
              "  12,\n",
              "  11,\n",
              "  10,\n",
              "  9,\n",
              "  8,\n",
              "  7,\n",
              "  6,\n",
              "  5,\n",
              "  4,\n",
              "  3,\n",
              "  2,\n",
              "  1]]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYJDfCmgCfUO"
      },
      "source": [
        "# Vectorize the data.\n",
        "# inwave = []\n",
        "# target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "# with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "#     lines = f.read().split(\"\\n\")\n",
        "for line in inwave:\n",
        "    # input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # # We use \"tab\" as the \"start sequence\" character\n",
        "    # # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    # target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    # input_texts.append(input_text)\n",
        "    # target_texts.append(target_text)\n",
        "    for char in line:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "for line in tarwave:            \n",
        "    for char in line:\n",
        "        if char not in target_characters:\n",
        "            target_characters.add(char)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8D8bDhhFhrw",
        "outputId": "f357377d-3c02-4007-cb98-60b4da29bd61"
      },
      "source": [
        "input_characters"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0,\n",
              " 1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24,\n",
              " 25,\n",
              " 26,\n",
              " 27,\n",
              " 28,\n",
              " 29,\n",
              " 30,\n",
              " 31,\n",
              " 32,\n",
              " 33,\n",
              " 34,\n",
              " 35,\n",
              " 36,\n",
              " 37,\n",
              " 38,\n",
              " 39,\n",
              " 40,\n",
              " 41,\n",
              " 42,\n",
              " 43,\n",
              " 44,\n",
              " 45,\n",
              " 46,\n",
              " 47,\n",
              " 48,\n",
              " 49,\n",
              " 50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 55,\n",
              " 56,\n",
              " 57,\n",
              " 58,\n",
              " 59,\n",
              " 60,\n",
              " 61,\n",
              " 62,\n",
              " 63,\n",
              " 64,\n",
              " 65,\n",
              " 66,\n",
              " 67,\n",
              " 68,\n",
              " 69,\n",
              " 70,\n",
              " 71,\n",
              " 72,\n",
              " 73,\n",
              " 74,\n",
              " 75,\n",
              " 76,\n",
              " 77,\n",
              " 78,\n",
              " 79,\n",
              " 80,\n",
              " 81,\n",
              " 82,\n",
              " 83,\n",
              " 84,\n",
              " 85,\n",
              " 86,\n",
              " 87,\n",
              " 88,\n",
              " 89,\n",
              " 90,\n",
              " 91,\n",
              " 92,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 96,\n",
              " 97,\n",
              " 98,\n",
              " 99,\n",
              " 100,\n",
              " 101,\n",
              " 102,\n",
              " 103,\n",
              " 104,\n",
              " 105,\n",
              " 106,\n",
              " 107,\n",
              " 108,\n",
              " 109,\n",
              " 110,\n",
              " 111,\n",
              " 112,\n",
              " 113,\n",
              " 114,\n",
              " 115,\n",
              " 116,\n",
              " 117,\n",
              " 118,\n",
              " 119,\n",
              " 120,\n",
              " 121,\n",
              " 122,\n",
              " 123,\n",
              " 124,\n",
              " 125,\n",
              " 126,\n",
              " 127,\n",
              " 128,\n",
              " 129,\n",
              " 130,\n",
              " 131,\n",
              " 132,\n",
              " 133,\n",
              " 134,\n",
              " 135,\n",
              " 136,\n",
              " 137,\n",
              " 138,\n",
              " 139,\n",
              " 140,\n",
              " 141,\n",
              " 142,\n",
              " 143,\n",
              " 144,\n",
              " 145,\n",
              " 146,\n",
              " 147,\n",
              " 148,\n",
              " 149,\n",
              " 150,\n",
              " 151,\n",
              " 152,\n",
              " 153,\n",
              " 154,\n",
              " 155,\n",
              " 156,\n",
              " 157,\n",
              " 158,\n",
              " 159,\n",
              " 160,\n",
              " 161,\n",
              " 162,\n",
              " 163,\n",
              " 164,\n",
              " 165,\n",
              " 166,\n",
              " 167,\n",
              " 168,\n",
              " 169,\n",
              " 170,\n",
              " 171,\n",
              " 172,\n",
              " 173,\n",
              " 174,\n",
              " 175,\n",
              " 176,\n",
              " 177,\n",
              " 178,\n",
              " 179,\n",
              " 180,\n",
              " 181,\n",
              " 182,\n",
              " 183,\n",
              " 184,\n",
              " 185,\n",
              " 186,\n",
              " 187,\n",
              " 188,\n",
              " 189,\n",
              " 190,\n",
              " 191,\n",
              " 192,\n",
              " 193,\n",
              " 194,\n",
              " 195,\n",
              " 196,\n",
              " 197,\n",
              " 198,\n",
              " 199,\n",
              " 200,\n",
              " 201,\n",
              " 202,\n",
              " 203,\n",
              " 204,\n",
              " 205,\n",
              " 206,\n",
              " 207,\n",
              " 208,\n",
              " 209,\n",
              " 210,\n",
              " 211,\n",
              " 212,\n",
              " 213,\n",
              " 214,\n",
              " 215,\n",
              " 216,\n",
              " 217,\n",
              " 218,\n",
              " 219,\n",
              " 220,\n",
              " 221,\n",
              " 222,\n",
              " 223,\n",
              " 224,\n",
              " 225,\n",
              " 226,\n",
              " 227,\n",
              " 228,\n",
              " 229,\n",
              " 230,\n",
              " 231,\n",
              " 232,\n",
              " 233,\n",
              " 234,\n",
              " 235,\n",
              " 236,\n",
              " 237,\n",
              " 238,\n",
              " 239,\n",
              " 240,\n",
              " 241,\n",
              " 242,\n",
              " 243,\n",
              " 244,\n",
              " 245,\n",
              " 246,\n",
              " 247,\n",
              " 248,\n",
              " 249,\n",
              " 250,\n",
              " 251,\n",
              " 252,\n",
              " 253,\n",
              " 254,\n",
              " 255,\n",
              " 256,\n",
              " 257,\n",
              " 258,\n",
              " 259,\n",
              " 260,\n",
              " 261,\n",
              " 262,\n",
              " 263,\n",
              " 264,\n",
              " 265,\n",
              " 266,\n",
              " 267,\n",
              " 268,\n",
              " 269,\n",
              " 270,\n",
              " 271,\n",
              " 272,\n",
              " 273,\n",
              " 274,\n",
              " 275,\n",
              " 276,\n",
              " 277,\n",
              " 278,\n",
              " 279,\n",
              " 280,\n",
              " 281,\n",
              " 282,\n",
              " 283,\n",
              " 284,\n",
              " 285,\n",
              " 286,\n",
              " 287,\n",
              " 288,\n",
              " 289,\n",
              " 290,\n",
              " 291,\n",
              " 292,\n",
              " 293,\n",
              " 294,\n",
              " 295,\n",
              " 296,\n",
              " 297,\n",
              " 298,\n",
              " 299,\n",
              " 300,\n",
              " 301,\n",
              " 302,\n",
              " 303,\n",
              " 304,\n",
              " 305,\n",
              " 306,\n",
              " 307,\n",
              " 308,\n",
              " 309,\n",
              " 310,\n",
              " 311,\n",
              " 312,\n",
              " 313,\n",
              " 314,\n",
              " 315,\n",
              " 316,\n",
              " 317,\n",
              " 318,\n",
              " 319,\n",
              " 320,\n",
              " 321,\n",
              " 322,\n",
              " 323,\n",
              " 324,\n",
              " 325,\n",
              " 326,\n",
              " 327,\n",
              " 328,\n",
              " 329,\n",
              " 330,\n",
              " 331,\n",
              " 332,\n",
              " 333,\n",
              " 334,\n",
              " 335,\n",
              " 336,\n",
              " 337,\n",
              " 338,\n",
              " 339,\n",
              " 340,\n",
              " 341,\n",
              " 342,\n",
              " 343,\n",
              " 344,\n",
              " 345,\n",
              " 346,\n",
              " 347,\n",
              " 348,\n",
              " 349,\n",
              " 350,\n",
              " 351,\n",
              " 352,\n",
              " 353,\n",
              " 354,\n",
              " 355,\n",
              " 356,\n",
              " 357,\n",
              " 358,\n",
              " 359,\n",
              " 360,\n",
              " 361,\n",
              " 362,\n",
              " 363,\n",
              " 364,\n",
              " 365,\n",
              " 366,\n",
              " 367,\n",
              " 368,\n",
              " 369,\n",
              " 370,\n",
              " 371,\n",
              " 372,\n",
              " 373,\n",
              " 374,\n",
              " 375,\n",
              " 376,\n",
              " 377,\n",
              " 378,\n",
              " 379,\n",
              " 380,\n",
              " 381,\n",
              " 382,\n",
              " 383,\n",
              " 384,\n",
              " 385,\n",
              " 386,\n",
              " 387,\n",
              " 388,\n",
              " 389,\n",
              " 390,\n",
              " 391,\n",
              " 392,\n",
              " 393,\n",
              " 394,\n",
              " 395,\n",
              " 396,\n",
              " 397,\n",
              " 398,\n",
              " 399,\n",
              " 400,\n",
              " 401,\n",
              " 402,\n",
              " 403,\n",
              " 404,\n",
              " 405,\n",
              " 406,\n",
              " 407,\n",
              " 408,\n",
              " 409,\n",
              " 410,\n",
              " 411,\n",
              " 412,\n",
              " 413,\n",
              " 414,\n",
              " 415,\n",
              " 416,\n",
              " 417,\n",
              " 418,\n",
              " 419,\n",
              " 420,\n",
              " 421,\n",
              " 422,\n",
              " 423,\n",
              " 424,\n",
              " 425,\n",
              " 426,\n",
              " 427,\n",
              " 428,\n",
              " 429,\n",
              " 430,\n",
              " 431,\n",
              " 432,\n",
              " 433,\n",
              " 434,\n",
              " 435,\n",
              " 436,\n",
              " 437,\n",
              " 438,\n",
              " 439,\n",
              " 440,\n",
              " 441,\n",
              " 442,\n",
              " 443,\n",
              " 444,\n",
              " 445,\n",
              " 446,\n",
              " 447,\n",
              " 448,\n",
              " 449,\n",
              " 450,\n",
              " 451,\n",
              " 452,\n",
              " 453,\n",
              " 454,\n",
              " 455,\n",
              " 456,\n",
              " 457,\n",
              " 458,\n",
              " 459,\n",
              " 460,\n",
              " 461,\n",
              " 462,\n",
              " 463,\n",
              " 464,\n",
              " 465,\n",
              " 466,\n",
              " 467,\n",
              " 468,\n",
              " 469,\n",
              " 470,\n",
              " 471,\n",
              " 472,\n",
              " 473,\n",
              " 474,\n",
              " 475,\n",
              " 476,\n",
              " 477,\n",
              " 478,\n",
              " 479,\n",
              " 480,\n",
              " 481,\n",
              " 482,\n",
              " 483,\n",
              " 484,\n",
              " 485,\n",
              " 486,\n",
              " 487,\n",
              " 488,\n",
              " 489,\n",
              " 490,\n",
              " 491,\n",
              " 492,\n",
              " 493,\n",
              " 494,\n",
              " 495,\n",
              " 496,\n",
              " 497,\n",
              " 498,\n",
              " 499,\n",
              " 500,\n",
              " 501,\n",
              " 502,\n",
              " 503,\n",
              " 504,\n",
              " 505,\n",
              " 506,\n",
              " 507,\n",
              " 508,\n",
              " 509,\n",
              " 510,\n",
              " 511,\n",
              " 512,\n",
              " 513,\n",
              " 514,\n",
              " 515,\n",
              " 516,\n",
              " 517,\n",
              " 518,\n",
              " 519,\n",
              " 520,\n",
              " 521,\n",
              " 522,\n",
              " 523,\n",
              " 524,\n",
              " 525,\n",
              " 526,\n",
              " 527,\n",
              " 528,\n",
              " 529,\n",
              " 530,\n",
              " 531,\n",
              " 532,\n",
              " 533,\n",
              " 534,\n",
              " 535,\n",
              " 536,\n",
              " 537,\n",
              " 538,\n",
              " 539,\n",
              " 540,\n",
              " 541,\n",
              " 542,\n",
              " 543,\n",
              " 544,\n",
              " 545,\n",
              " 546,\n",
              " 547,\n",
              " 548,\n",
              " 549,\n",
              " 550,\n",
              " 551,\n",
              " 552,\n",
              " 553,\n",
              " 554,\n",
              " 555,\n",
              " 556,\n",
              " 557,\n",
              " 558,\n",
              " 559,\n",
              " 560,\n",
              " 561,\n",
              " 562,\n",
              " 563,\n",
              " 564,\n",
              " 565,\n",
              " 566,\n",
              " 567,\n",
              " 568,\n",
              " 569,\n",
              " 570,\n",
              " 571,\n",
              " 572,\n",
              " 573,\n",
              " 574,\n",
              " 575,\n",
              " 576,\n",
              " 577,\n",
              " 578,\n",
              " 579,\n",
              " 580,\n",
              " 581,\n",
              " 582,\n",
              " 583,\n",
              " 584,\n",
              " 585,\n",
              " 586,\n",
              " 587,\n",
              " 588,\n",
              " 589,\n",
              " 590,\n",
              " 591,\n",
              " 592,\n",
              " 593,\n",
              " 594,\n",
              " 595,\n",
              " 596,\n",
              " 597,\n",
              " 598,\n",
              " 599,\n",
              " 600,\n",
              " 601,\n",
              " 602,\n",
              " 603,\n",
              " 604,\n",
              " 605,\n",
              " 606,\n",
              " 607,\n",
              " 608,\n",
              " 609,\n",
              " 610,\n",
              " 611,\n",
              " 612,\n",
              " 613,\n",
              " 614,\n",
              " 615,\n",
              " 616,\n",
              " 617,\n",
              " 618,\n",
              " 619,\n",
              " 620,\n",
              " 621,\n",
              " 622,\n",
              " 623,\n",
              " 624,\n",
              " 625,\n",
              " 626,\n",
              " 627,\n",
              " 628,\n",
              " 629,\n",
              " 630,\n",
              " 631,\n",
              " 632,\n",
              " 633,\n",
              " 634,\n",
              " 635,\n",
              " 636,\n",
              " 637,\n",
              " 638,\n",
              " 639,\n",
              " 640,\n",
              " 641,\n",
              " 642,\n",
              " 643,\n",
              " 644,\n",
              " 645,\n",
              " 646,\n",
              " 647,\n",
              " 648,\n",
              " 649,\n",
              " 650,\n",
              " 651,\n",
              " 652,\n",
              " 653,\n",
              " 654,\n",
              " 655,\n",
              " 656,\n",
              " 657,\n",
              " 658,\n",
              " 659,\n",
              " 660,\n",
              " 661,\n",
              " 662,\n",
              " 663,\n",
              " 664,\n",
              " 665,\n",
              " 666,\n",
              " 667,\n",
              " 668,\n",
              " 669,\n",
              " 670,\n",
              " 671,\n",
              " 672,\n",
              " 673,\n",
              " 674,\n",
              " 675,\n",
              " 676,\n",
              " 677,\n",
              " 678,\n",
              " 679,\n",
              " 680,\n",
              " 681,\n",
              " 682,\n",
              " 683,\n",
              " 684,\n",
              " 685,\n",
              " 686,\n",
              " 687,\n",
              " 688,\n",
              " 689,\n",
              " 690,\n",
              " 691,\n",
              " 692,\n",
              " 693,\n",
              " 694,\n",
              " 695,\n",
              " 696,\n",
              " 697,\n",
              " 698,\n",
              " 699,\n",
              " 700,\n",
              " 701,\n",
              " 702,\n",
              " 703,\n",
              " 704,\n",
              " 705,\n",
              " 706,\n",
              " 707,\n",
              " 708,\n",
              " 709,\n",
              " 710,\n",
              " 711,\n",
              " 712,\n",
              " 713,\n",
              " 714,\n",
              " 715,\n",
              " 716,\n",
              " 717,\n",
              " 718,\n",
              " 719,\n",
              " 720,\n",
              " 721,\n",
              " 722,\n",
              " 723,\n",
              " 724,\n",
              " 725,\n",
              " 726,\n",
              " 727,\n",
              " 728,\n",
              " 729,\n",
              " 730,\n",
              " 731,\n",
              " 732,\n",
              " 733,\n",
              " 734,\n",
              " 735,\n",
              " 736,\n",
              " 737,\n",
              " 738,\n",
              " 739,\n",
              " 740,\n",
              " 741,\n",
              " 742,\n",
              " 743,\n",
              " 744,\n",
              " 745,\n",
              " 746,\n",
              " 747,\n",
              " 748,\n",
              " 749,\n",
              " 750,\n",
              " 751,\n",
              " 752,\n",
              " 753,\n",
              " 754,\n",
              " 755,\n",
              " 756,\n",
              " 757,\n",
              " 758,\n",
              " 759,\n",
              " 760,\n",
              " 761,\n",
              " 762,\n",
              " 763,\n",
              " 764,\n",
              " 765,\n",
              " 766,\n",
              " 767,\n",
              " 768,\n",
              " 769,\n",
              " 770,\n",
              " 771,\n",
              " 772,\n",
              " 773,\n",
              " 774,\n",
              " 775,\n",
              " 776,\n",
              " 777,\n",
              " 778,\n",
              " 779,\n",
              " 780,\n",
              " 781,\n",
              " 782,\n",
              " 783,\n",
              " 784,\n",
              " 785,\n",
              " 786,\n",
              " 787,\n",
              " 788,\n",
              " 789,\n",
              " 790,\n",
              " 791,\n",
              " 792,\n",
              " 793,\n",
              " 794,\n",
              " 795,\n",
              " 796,\n",
              " 797,\n",
              " 798,\n",
              " 799,\n",
              " 800,\n",
              " 801,\n",
              " 802,\n",
              " 803,\n",
              " 804,\n",
              " 805,\n",
              " 806,\n",
              " 807,\n",
              " 808,\n",
              " 809,\n",
              " 810,\n",
              " 811,\n",
              " 812,\n",
              " 813,\n",
              " 814,\n",
              " 815,\n",
              " 816,\n",
              " 817,\n",
              " 818,\n",
              " 819,\n",
              " 820,\n",
              " 821,\n",
              " 822,\n",
              " 823,\n",
              " 824,\n",
              " 825,\n",
              " 826,\n",
              " 827,\n",
              " 828,\n",
              " 829,\n",
              " 830,\n",
              " 831,\n",
              " 832,\n",
              " 833,\n",
              " 834,\n",
              " 835,\n",
              " 836,\n",
              " 837,\n",
              " 838,\n",
              " 839,\n",
              " 840,\n",
              " 841,\n",
              " 842,\n",
              " 843,\n",
              " 844,\n",
              " 845,\n",
              " 846,\n",
              " 847,\n",
              " 848,\n",
              " 849,\n",
              " 850,\n",
              " 851,\n",
              " 852,\n",
              " 853,\n",
              " 854,\n",
              " 855,\n",
              " 856,\n",
              " 857,\n",
              " 858,\n",
              " 859,\n",
              " 860,\n",
              " 861,\n",
              " 862,\n",
              " 863,\n",
              " 864,\n",
              " 865,\n",
              " 866,\n",
              " 867,\n",
              " 868,\n",
              " 869,\n",
              " 870,\n",
              " 871,\n",
              " 872,\n",
              " 873,\n",
              " 874,\n",
              " 875,\n",
              " 876,\n",
              " 877,\n",
              " 878,\n",
              " 879,\n",
              " 880,\n",
              " 881,\n",
              " 882,\n",
              " 883,\n",
              " 884,\n",
              " 885,\n",
              " 886,\n",
              " 887,\n",
              " 888,\n",
              " 889,\n",
              " 890,\n",
              " 891,\n",
              " 892,\n",
              " 893,\n",
              " 894,\n",
              " 895,\n",
              " 896,\n",
              " 897,\n",
              " 898,\n",
              " 899,\n",
              " 900,\n",
              " 901,\n",
              " 902,\n",
              " 903,\n",
              " 904,\n",
              " 905,\n",
              " 906,\n",
              " 907,\n",
              " 908,\n",
              " 909,\n",
              " 910,\n",
              " 911,\n",
              " 912,\n",
              " 913,\n",
              " 914,\n",
              " 915,\n",
              " 916,\n",
              " 917,\n",
              " 918,\n",
              " 919,\n",
              " 920,\n",
              " 921,\n",
              " 922,\n",
              " 923,\n",
              " 924,\n",
              " 925,\n",
              " 926,\n",
              " 927,\n",
              " 928,\n",
              " 929,\n",
              " 930,\n",
              " 931,\n",
              " 932,\n",
              " 933,\n",
              " 934,\n",
              " 935,\n",
              " 936,\n",
              " 937,\n",
              " 938,\n",
              " 939,\n",
              " 940,\n",
              " 941,\n",
              " 942,\n",
              " 943,\n",
              " 944,\n",
              " 945,\n",
              " 946,\n",
              " 947,\n",
              " 948,\n",
              " 949,\n",
              " 950,\n",
              " 951,\n",
              " 952,\n",
              " 953,\n",
              " 954,\n",
              " 955,\n",
              " 956,\n",
              " 957,\n",
              " 958,\n",
              " 959,\n",
              " 960,\n",
              " 961,\n",
              " 962,\n",
              " 963,\n",
              " 964,\n",
              " 965,\n",
              " 966,\n",
              " 967,\n",
              " 968,\n",
              " 969,\n",
              " 970,\n",
              " 971,\n",
              " 972,\n",
              " 973,\n",
              " 974,\n",
              " 975,\n",
              " 976,\n",
              " 977,\n",
              " 978,\n",
              " 979,\n",
              " 980,\n",
              " 981,\n",
              " 982,\n",
              " 983,\n",
              " 984,\n",
              " 985,\n",
              " 986,\n",
              " 987,\n",
              " 988,\n",
              " 989,\n",
              " 990,\n",
              " 991,\n",
              " 992,\n",
              " 993,\n",
              " 994,\n",
              " 995,\n",
              " 996,\n",
              " 997,\n",
              " 998,\n",
              " 999}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69psgZ0ZF_tx",
        "outputId": "e258e215-2d19-49ee-80a1-14ba039dfa55"
      },
      "source": [
        "len(inwave)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J32QgnqUF0bO"
      },
      "source": [
        "Character encodind done ok!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZizlYvk5lo5-",
        "outputId": "b272b9c7-bdf7-46dd-fc1d-bc74dfa374d5"
      },
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in inwave])\n",
        "max_decoder_seq_length = max([len(txt) for txt in tarwave])\n",
        "\n",
        "print(\"Number of samples:\", len(inwave))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(inwave), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(inwave), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(inwave), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
        ")\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(inwave, tarwave)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[0]] = 1.0\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[1]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[1]] = 1.0"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 20\n",
            "Number of unique input tokens: 1000\n",
            "Number of unique output tokens: 1000\n",
            "Max sequence length for inputs: 50\n",
            "Max sequence length for outputs: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb1L9XWvDrDn"
      },
      "source": [
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6dzuDeXCfUQ"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1URGpVWACfUQ"
      },
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUgOGMHbCfUR"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mm_DIcDCfUR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c14a32d0-5b74-455e-a9df-8a7096e318b6"
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "history = model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=1000,\n",
        "    validation_split=0.2,\n",
        ")\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# Save model\n",
        "model.save(\"s2s\")\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 5s 5s/step - loss: 6.9079 - accuracy: 0.0050 - val_loss: 6.9122 - val_accuracy: 0.0100\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 6.8851 - accuracy: 0.1963 - val_loss: 6.9206 - val_accuracy: 0.0250\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 6.8636 - accuracy: 0.1963 - val_loss: 6.9689 - val_accuracy: 0.0250\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 6.7830 - accuracy: 0.0275 - val_loss: 8.0729 - val_accuracy: 0.0250\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 6.6859 - accuracy: 0.0225 - val_loss: 7.7541 - val_accuracy: 0.0250\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 6.4698 - accuracy: 0.0237 - val_loss: 8.1490 - val_accuracy: 0.0250\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 6.2523 - accuracy: 0.0237 - val_loss: 8.3144 - val_accuracy: 0.0250\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 6.0781 - accuracy: 0.0250 - val_loss: 8.5927 - val_accuracy: 0.0250\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 6.4801 - accuracy: 0.0237 - val_loss: 8.7213 - val_accuracy: 0.0250\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 6.2799 - accuracy: 0.0225 - val_loss: 8.8736 - val_accuracy: 0.0250\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 6.0851 - accuracy: 0.0237 - val_loss: 8.9676 - val_accuracy: 0.0250\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 5.9375 - accuracy: 0.0237 - val_loss: 8.8993 - val_accuracy: 0.0250\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 5.8337 - accuracy: 0.0237 - val_loss: 9.0615 - val_accuracy: 0.0250\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 5.8327 - accuracy: 0.0312 - val_loss: 9.1619 - val_accuracy: 0.0250\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 159ms/step - loss: 5.8800 - accuracy: 0.0225 - val_loss: 9.0647 - val_accuracy: 0.0250\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 5.9078 - accuracy: 0.0275 - val_loss: 9.0412 - val_accuracy: 0.0250\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 5.6630 - accuracy: 0.0250 - val_loss: 9.3677 - val_accuracy: 0.0250\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 5.5852 - accuracy: 0.0500 - val_loss: 9.5009 - val_accuracy: 0.0250\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 5.5289 - accuracy: 0.0338 - val_loss: 9.5020 - val_accuracy: 0.0250\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 5.5766 - accuracy: 0.0512 - val_loss: 9.6770 - val_accuracy: 0.0250\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 5.6118 - accuracy: 0.0250 - val_loss: 9.6470 - val_accuracy: 0.0250\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 5.7069 - accuracy: 0.0375 - val_loss: 9.5883 - val_accuracy: 0.0250\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 5.4162 - accuracy: 0.0275 - val_loss: 9.8967 - val_accuracy: 0.0250\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 5.3511 - accuracy: 0.0913 - val_loss: 10.0383 - val_accuracy: 0.0250\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 5.2944 - accuracy: 0.0425 - val_loss: 9.9839 - val_accuracy: 0.0250\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 5.3160 - accuracy: 0.0887 - val_loss: 10.2505 - val_accuracy: 0.0250\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 5.3817 - accuracy: 0.0300 - val_loss: 10.0949 - val_accuracy: 0.0250\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 5.5788 - accuracy: 0.0463 - val_loss: 9.8429 - val_accuracy: 0.0250\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 5.2909 - accuracy: 0.0225 - val_loss: 10.3060 - val_accuracy: 0.0250\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 5.2269 - accuracy: 0.0575 - val_loss: 10.4740 - val_accuracy: 0.0250\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 5.1370 - accuracy: 0.0512 - val_loss: 10.3769 - val_accuracy: 0.0250\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 5.1581 - accuracy: 0.1225 - val_loss: 10.5863 - val_accuracy: 0.0250\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 5.1598 - accuracy: 0.0450 - val_loss: 10.5111 - val_accuracy: 0.0250\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 5.3081 - accuracy: 0.0750 - val_loss: 10.5767 - val_accuracy: 0.0250\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 5.1541 - accuracy: 0.0275 - val_loss: 10.5976 - val_accuracy: 0.0250\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 5.1796 - accuracy: 0.0475 - val_loss: 10.8728 - val_accuracy: 0.0250\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 5.0298 - accuracy: 0.0487 - val_loss: 10.6918 - val_accuracy: 0.0250\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 5.0437 - accuracy: 0.1425 - val_loss: 10.7474 - val_accuracy: 0.0250\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 4.9817 - accuracy: 0.0662 - val_loss: 11.1484 - val_accuracy: 0.0250\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 4.9809 - accuracy: 0.1600 - val_loss: 11.2684 - val_accuracy: 0.0250\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 4.9530 - accuracy: 0.0600 - val_loss: 10.7706 - val_accuracy: 0.0250\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 5.0778 - accuracy: 0.0988 - val_loss: 10.8949 - val_accuracy: 0.0250\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 5.0093 - accuracy: 0.0375 - val_loss: 11.1701 - val_accuracy: 0.0250\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 5.0637 - accuracy: 0.0525 - val_loss: 11.3504 - val_accuracy: 0.0250\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 4.9030 - accuracy: 0.0450 - val_loss: 11.0504 - val_accuracy: 0.0250\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 4.8904 - accuracy: 0.1600 - val_loss: 11.2760 - val_accuracy: 0.0250\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 4.7888 - accuracy: 0.1412 - val_loss: 11.3906 - val_accuracy: 0.0250\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 4.7545 - accuracy: 0.2962 - val_loss: 11.4964 - val_accuracy: 0.0250\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 4.7374 - accuracy: 0.1750 - val_loss: 11.5614 - val_accuracy: 0.0250\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 4.7762 - accuracy: 0.2425 - val_loss: 11.8388 - val_accuracy: 0.0250\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 4.8406 - accuracy: 0.1100 - val_loss: 10.9915 - val_accuracy: 0.0250\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 5.0595 - accuracy: 0.1088 - val_loss: 10.8205 - val_accuracy: 0.0250\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 4.8566 - accuracy: 0.0550 - val_loss: 11.8062 - val_accuracy: 0.0250\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 4.8020 - accuracy: 0.0575 - val_loss: 11.6292 - val_accuracy: 0.0250\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 4.6438 - accuracy: 0.1725 - val_loss: 11.7165 - val_accuracy: 0.0250\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 4.6127 - accuracy: 0.2925 - val_loss: 11.9514 - val_accuracy: 0.0250\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 4.5986 - accuracy: 0.2700 - val_loss: 11.7282 - val_accuracy: 0.0250\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 4.6666 - accuracy: 0.3100 - val_loss: 11.9924 - val_accuracy: 0.0250\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 4.6775 - accuracy: 0.1737 - val_loss: 11.7039 - val_accuracy: 0.0250\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 4.8247 - accuracy: 0.1838 - val_loss: 11.7848 - val_accuracy: 0.0250\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 4.6708 - accuracy: 0.0938 - val_loss: 11.8054 - val_accuracy: 0.0250\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 4.6336 - accuracy: 0.0913 - val_loss: 12.1161 - val_accuracy: 0.0250\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 4.4887 - accuracy: 0.1900 - val_loss: 11.7906 - val_accuracy: 0.0250\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 4.4754 - accuracy: 0.3075 - val_loss: 12.6735 - val_accuracy: 0.0250\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 4.4316 - accuracy: 0.3262 - val_loss: 12.1333 - val_accuracy: 0.0250\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 4.4263 - accuracy: 0.3875 - val_loss: 12.1459 - val_accuracy: 0.0250\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 4.4863 - accuracy: 0.2425 - val_loss: 12.2914 - val_accuracy: 0.0250\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 4.8102 - accuracy: 0.2087 - val_loss: 12.0223 - val_accuracy: 0.0250\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 4.7348 - accuracy: 0.1338 - val_loss: 11.8409 - val_accuracy: 0.0250\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 4.7241 - accuracy: 0.1200 - val_loss: 11.8451 - val_accuracy: 0.0250\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 4.4513 - accuracy: 0.2362 - val_loss: 12.2592 - val_accuracy: 0.0250\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 4.3499 - accuracy: 0.3512 - val_loss: 12.2528 - val_accuracy: 0.0250\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 4.2661 - accuracy: 0.5138 - val_loss: 12.3485 - val_accuracy: 0.0250\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 4.2662 - accuracy: 0.4875 - val_loss: 12.7919 - val_accuracy: 0.0250\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 4.2329 - accuracy: 0.5175 - val_loss: 11.9273 - val_accuracy: 0.0250\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 4.3723 - accuracy: 0.4062 - val_loss: 13.4176 - val_accuracy: 0.0250\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 4.4131 - accuracy: 0.2050 - val_loss: 12.6545 - val_accuracy: 0.0250\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 4.4998 - accuracy: 0.2600 - val_loss: 11.9412 - val_accuracy: 0.0250\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 4.3775 - accuracy: 0.1912 - val_loss: 12.6364 - val_accuracy: 0.0250\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 4.2952 - accuracy: 0.1450 - val_loss: 12.4807 - val_accuracy: 0.0250\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 4.2039 - accuracy: 0.2912 - val_loss: 12.6100 - val_accuracy: 0.0250\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 4.2105 - accuracy: 0.3025 - val_loss: 12.6579 - val_accuracy: 0.0250\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 4.0834 - accuracy: 0.5650 - val_loss: 12.9534 - val_accuracy: 0.0250\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 3.9982 - accuracy: 0.6488 - val_loss: 13.0542 - val_accuracy: 0.0250\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 4.0528 - accuracy: 0.4625 - val_loss: 12.7615 - val_accuracy: 0.0250\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 4.0720 - accuracy: 0.3738 - val_loss: 12.8991 - val_accuracy: 0.0250\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 4.0444 - accuracy: 0.3725 - val_loss: 13.1707 - val_accuracy: 0.0250\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 4.1961 - accuracy: 0.2537 - val_loss: 13.1011 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 4.6165 - accuracy: 0.1450 - val_loss: 11.6743 - val_accuracy: 0.0250\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 4.4202 - accuracy: 0.2225 - val_loss: 11.2083 - val_accuracy: 0.0250\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 4.1933 - accuracy: 0.3988 - val_loss: 13.1071 - val_accuracy: 0.0250\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 3.9839 - accuracy: 0.3537 - val_loss: 12.7608 - val_accuracy: 0.0250\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 3.8867 - accuracy: 0.5950 - val_loss: 12.6487 - val_accuracy: 0.0250\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 3.9181 - accuracy: 0.5125 - val_loss: 13.3795 - val_accuracy: 0.0250\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 3.8652 - accuracy: 0.5788 - val_loss: 12.8668 - val_accuracy: 0.0250\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 3.8578 - accuracy: 0.5150 - val_loss: 13.1152 - val_accuracy: 0.0200\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 3.9008 - accuracy: 0.5088 - val_loss: 12.9572 - val_accuracy: 0.0250\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 3.9250 - accuracy: 0.4200 - val_loss: 12.4500 - val_accuracy: 0.0250\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 3.8100 - accuracy: 0.4437 - val_loss: 13.7593 - val_accuracy: 0.0250\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 3.9649 - accuracy: 0.1750 - val_loss: 13.1871 - val_accuracy: 0.0250\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 4.1166 - accuracy: 0.1538 - val_loss: 12.8190 - val_accuracy: 0.0250\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 3.8730 - accuracy: 0.2100 - val_loss: 12.8562 - val_accuracy: 0.0250\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 3.7000 - accuracy: 0.4663 - val_loss: 13.7181 - val_accuracy: 0.0250\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 3.6251 - accuracy: 0.4737 - val_loss: 13.0304 - val_accuracy: 0.0250\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 3.6450 - accuracy: 0.6438 - val_loss: 13.0076 - val_accuracy: 0.0250\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 3.4595 - accuracy: 0.6650 - val_loss: 14.1181 - val_accuracy: 0.0250\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 3.4323 - accuracy: 0.6525 - val_loss: 12.9265 - val_accuracy: 0.0250\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 3.5706 - accuracy: 0.5013 - val_loss: 13.2453 - val_accuracy: 0.0250\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 3.6002 - accuracy: 0.5025 - val_loss: 12.9752 - val_accuracy: 0.0250\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 3.5177 - accuracy: 0.5825 - val_loss: 13.6996 - val_accuracy: 0.0250\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 3.3555 - accuracy: 0.6988 - val_loss: 13.0899 - val_accuracy: 0.0250\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 3.5499 - accuracy: 0.4525 - val_loss: 13.3993 - val_accuracy: 0.0250\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 3.3594 - accuracy: 0.4487 - val_loss: 13.9720 - val_accuracy: 0.0250\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 3.5847 - accuracy: 0.2237 - val_loss: 13.0639 - val_accuracy: 0.0250\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 3.6849 - accuracy: 0.2412 - val_loss: 12.7850 - val_accuracy: 0.0250\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 3.5090 - accuracy: 0.2537 - val_loss: 13.4791 - val_accuracy: 0.0250\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 3.1517 - accuracy: 0.6913 - val_loss: 14.1150 - val_accuracy: 0.0250\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 3.0488 - accuracy: 0.6400 - val_loss: 13.8141 - val_accuracy: 0.0250\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 3.1173 - accuracy: 0.6913 - val_loss: 13.8204 - val_accuracy: 0.0250\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 3.1568 - accuracy: 0.5850 - val_loss: 13.8657 - val_accuracy: 0.0250\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 2.9729 - accuracy: 0.7713 - val_loss: 13.5766 - val_accuracy: 0.0250\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 3.1059 - accuracy: 0.5487 - val_loss: 13.4647 - val_accuracy: 0.0250\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.9871 - accuracy: 0.7400 - val_loss: 14.6569 - val_accuracy: 0.0250\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 3.0382 - accuracy: 0.4913 - val_loss: 12.5884 - val_accuracy: 0.0250\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 3.1807 - accuracy: 0.6075 - val_loss: 14.2230 - val_accuracy: 0.0250\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.7912 - accuracy: 0.7063 - val_loss: 14.4353 - val_accuracy: 0.0250\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.8289 - accuracy: 0.6475 - val_loss: 13.8213 - val_accuracy: 0.0250\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.9801 - accuracy: 0.4487 - val_loss: 14.2588 - val_accuracy: 0.0250\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 2.7999 - accuracy: 0.5375 - val_loss: 13.4538 - val_accuracy: 0.0250\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 2.9844 - accuracy: 0.4425 - val_loss: 13.8650 - val_accuracy: 0.0250\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 2.6731 - accuracy: 0.6750 - val_loss: 14.6436 - val_accuracy: 0.0250\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.8029 - accuracy: 0.4000 - val_loss: 14.5618 - val_accuracy: 0.0250\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 3.1729 - accuracy: 0.2425 - val_loss: 12.7026 - val_accuracy: 0.0250\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 2.9580 - accuracy: 0.4525 - val_loss: 13.8948 - val_accuracy: 0.0250\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.5257 - accuracy: 0.8238 - val_loss: 14.6282 - val_accuracy: 0.0250\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 2.4171 - accuracy: 0.7912 - val_loss: 14.8113 - val_accuracy: 0.0250\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.3561 - accuracy: 0.8562 - val_loss: 15.0420 - val_accuracy: 0.0250\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 2.3382 - accuracy: 0.7538 - val_loss: 15.2902 - val_accuracy: 0.0250\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.2967 - accuracy: 0.7437 - val_loss: 15.1481 - val_accuracy: 0.0250\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 2.3419 - accuracy: 0.5250 - val_loss: 15.5149 - val_accuracy: 0.0250\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 2.4297 - accuracy: 0.4475 - val_loss: 14.4556 - val_accuracy: 0.0250\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 2.6978 - accuracy: 0.3113 - val_loss: 13.3858 - val_accuracy: 0.0250\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 2.4601 - accuracy: 0.6200 - val_loss: 16.0275 - val_accuracy: 0.0250\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 2.3251 - accuracy: 0.5825 - val_loss: 14.4555 - val_accuracy: 0.0250\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 2.2572 - accuracy: 0.7337 - val_loss: 15.6851 - val_accuracy: 0.0250\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 2.2223 - accuracy: 0.6775 - val_loss: 15.2810 - val_accuracy: 0.0250\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.2748 - accuracy: 0.6075 - val_loss: 14.9104 - val_accuracy: 0.0250\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 2.3766 - accuracy: 0.4575 - val_loss: 14.6968 - val_accuracy: 0.0250\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 2.1926 - accuracy: 0.5913 - val_loss: 15.0580 - val_accuracy: 0.0250\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 2.2990 - accuracy: 0.4663 - val_loss: 14.6942 - val_accuracy: 0.0250\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 2.1199 - accuracy: 0.6787 - val_loss: 15.5162 - val_accuracy: 0.0250\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 2.0125 - accuracy: 0.7588 - val_loss: 15.5959 - val_accuracy: 0.0250\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.9470 - accuracy: 0.8475 - val_loss: 15.2827 - val_accuracy: 0.0250\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.9763 - accuracy: 0.7138 - val_loss: 15.8694 - val_accuracy: 0.0250\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 1.9678 - accuracy: 0.6875 - val_loss: 15.1692 - val_accuracy: 0.0250\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 2.0948 - accuracy: 0.5013 - val_loss: 14.9215 - val_accuracy: 0.0250\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 2.0917 - accuracy: 0.5587 - val_loss: 15.1413 - val_accuracy: 0.0250\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 2.2539 - accuracy: 0.5650 - val_loss: 14.7635 - val_accuracy: 0.0250\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.9734 - accuracy: 0.8263 - val_loss: 15.7242 - val_accuracy: 0.0250\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.8401 - accuracy: 0.8200 - val_loss: 15.5796 - val_accuracy: 0.0250\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 1.8231 - accuracy: 0.7538 - val_loss: 16.0718 - val_accuracy: 0.0250\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 1.9045 - accuracy: 0.4588 - val_loss: 15.2484 - val_accuracy: 0.0250\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.9530 - accuracy: 0.3950 - val_loss: 15.1598 - val_accuracy: 0.0250\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.9889 - accuracy: 0.3525 - val_loss: 14.5870 - val_accuracy: 0.0250\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.8330 - accuracy: 0.6938 - val_loss: 15.9396 - val_accuracy: 0.0250\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.7849 - accuracy: 0.7650 - val_loss: 15.7628 - val_accuracy: 0.0250\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 1.7113 - accuracy: 0.9075 - val_loss: 16.2188 - val_accuracy: 0.0250\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.6583 - accuracy: 0.9162 - val_loss: 16.4452 - val_accuracy: 0.0250\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.6177 - accuracy: 0.9563 - val_loss: 16.0991 - val_accuracy: 0.0250\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.6194 - accuracy: 0.9325 - val_loss: 16.6897 - val_accuracy: 0.0250\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.6098 - accuracy: 0.8737 - val_loss: 15.9588 - val_accuracy: 0.0250\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.7375 - accuracy: 0.5850 - val_loss: 15.9510 - val_accuracy: 0.0250\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.9097 - accuracy: 0.3438 - val_loss: 15.1239 - val_accuracy: 0.0250\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 2.1792 - accuracy: 0.3638 - val_loss: 12.5841 - val_accuracy: 0.0000e+00\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 2.0237 - accuracy: 0.5238 - val_loss: 14.7110 - val_accuracy: 0.0250\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.8028 - accuracy: 0.7175 - val_loss: 16.0037 - val_accuracy: 0.0250\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.5324 - accuracy: 0.9375 - val_loss: 15.6613 - val_accuracy: 0.0250\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.4863 - accuracy: 0.9688 - val_loss: 16.5290 - val_accuracy: 0.0250\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.4556 - accuracy: 0.9775 - val_loss: 16.2071 - val_accuracy: 0.0250\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.4635 - accuracy: 0.9187 - val_loss: 16.5185 - val_accuracy: 0.0250\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.4589 - accuracy: 0.8975 - val_loss: 16.4118 - val_accuracy: 0.0250\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.5081 - accuracy: 0.6862 - val_loss: 15.6223 - val_accuracy: 0.0250\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 1.5232 - accuracy: 0.6488 - val_loss: 16.3379 - val_accuracy: 0.0250\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 1.5852 - accuracy: 0.4700 - val_loss: 15.0588 - val_accuracy: 0.0250\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.6179 - accuracy: 0.5437 - val_loss: 16.2640 - val_accuracy: 0.0250\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.5813 - accuracy: 0.6288 - val_loss: 16.1240 - val_accuracy: 0.0250\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.4935 - accuracy: 0.7225 - val_loss: 16.3870 - val_accuracy: 0.0250\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 1.5420 - accuracy: 0.6612 - val_loss: 15.9227 - val_accuracy: 0.0250\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 1.4831 - accuracy: 0.7150 - val_loss: 16.1479 - val_accuracy: 0.0250\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.4926 - accuracy: 0.6513 - val_loss: 16.0753 - val_accuracy: 0.0250\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.4460 - accuracy: 0.6787 - val_loss: 15.3554 - val_accuracy: 0.0250\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.4524 - accuracy: 0.7050 - val_loss: 15.4851 - val_accuracy: 0.0250\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 1.3379 - accuracy: 0.9413 - val_loss: 16.4739 - val_accuracy: 0.0250\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 1.2846 - accuracy: 0.9613 - val_loss: 16.2534 - val_accuracy: 0.0250\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.2605 - accuracy: 0.9862 - val_loss: 16.9304 - val_accuracy: 0.0250\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.2606 - accuracy: 0.9438 - val_loss: 16.3427 - val_accuracy: 0.0250\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.2755 - accuracy: 0.9237 - val_loss: 16.9096 - val_accuracy: 0.0250\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.3374 - accuracy: 0.7550 - val_loss: 15.7643 - val_accuracy: 0.0250\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.4004 - accuracy: 0.6237 - val_loss: 16.2753 - val_accuracy: 0.0250\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 1.5202 - accuracy: 0.4387 - val_loss: 15.0899 - val_accuracy: 0.0250\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.5859 - accuracy: 0.4875 - val_loss: 15.4864 - val_accuracy: 0.0250\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 1.5507 - accuracy: 0.6012 - val_loss: 15.7800 - val_accuracy: 0.0250\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.3209 - accuracy: 0.8913 - val_loss: 16.4893 - val_accuracy: 0.0250\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.2558 - accuracy: 0.9488 - val_loss: 16.1985 - val_accuracy: 0.0250\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.2067 - accuracy: 0.9362 - val_loss: 16.8159 - val_accuracy: 0.0250\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 1.1943 - accuracy: 0.9475 - val_loss: 16.6191 - val_accuracy: 0.0250\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 1.1780 - accuracy: 0.9237 - val_loss: 16.5900 - val_accuracy: 0.0250\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.2546 - accuracy: 0.7450 - val_loss: 15.2998 - val_accuracy: 0.0250\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.2798 - accuracy: 0.7575 - val_loss: 15.9387 - val_accuracy: 0.0250\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.2186 - accuracy: 0.8612 - val_loss: 15.7504 - val_accuracy: 0.0250\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 1.1156 - accuracy: 0.9987 - val_loss: 16.8701 - val_accuracy: 0.0250\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.0752 - accuracy: 0.9825 - val_loss: 16.4578 - val_accuracy: 0.0250\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.0726 - accuracy: 0.9937 - val_loss: 17.0623 - val_accuracy: 0.0250\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.0983 - accuracy: 0.9588 - val_loss: 16.2359 - val_accuracy: 0.0250\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.1871 - accuracy: 0.8087 - val_loss: 16.4773 - val_accuracy: 0.0250\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 1.3138 - accuracy: 0.6100 - val_loss: 15.5157 - val_accuracy: 0.0250\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.3961 - accuracy: 0.5312 - val_loss: 16.3264 - val_accuracy: 0.0250\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.4168 - accuracy: 0.5788 - val_loss: 15.2370 - val_accuracy: 0.0250\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.1963 - accuracy: 0.8675 - val_loss: 15.6271 - val_accuracy: 0.0250\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.0998 - accuracy: 0.9875 - val_loss: 16.8350 - val_accuracy: 0.0250\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.0379 - accuracy: 0.9762 - val_loss: 16.2564 - val_accuracy: 0.0250\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.0476 - accuracy: 0.9675 - val_loss: 16.4452 - val_accuracy: 0.0250\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 1.0259 - accuracy: 0.9825 - val_loss: 16.6020 - val_accuracy: 0.0250\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.0645 - accuracy: 0.9175 - val_loss: 15.5580 - val_accuracy: 0.0200\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.0948 - accuracy: 0.8838 - val_loss: 16.1341 - val_accuracy: 0.0250\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.0541 - accuracy: 0.9262 - val_loss: 15.6920 - val_accuracy: 0.0250\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 1.0016 - accuracy: 0.9862 - val_loss: 16.6431 - val_accuracy: 0.0250\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.9636 - accuracy: 0.9925 - val_loss: 16.5061 - val_accuracy: 0.0250\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.9380 - accuracy: 0.9987 - val_loss: 17.0378 - val_accuracy: 0.0250\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.9294 - accuracy: 0.9862 - val_loss: 16.4676 - val_accuracy: 0.0250\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.9545 - accuracy: 0.9825 - val_loss: 16.9277 - val_accuracy: 0.0250\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 1.0621 - accuracy: 0.8525 - val_loss: 15.7710 - val_accuracy: 0.0250\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 1.3566 - accuracy: 0.5512 - val_loss: 16.3169 - val_accuracy: 0.0250\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 1.3859 - accuracy: 0.5163 - val_loss: 14.7983 - val_accuracy: 0.0250\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.2180 - accuracy: 0.7150 - val_loss: 16.2701 - val_accuracy: 0.0250\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 1.0486 - accuracy: 0.9438 - val_loss: 16.5667 - val_accuracy: 0.0250\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.9025 - accuracy: 0.9987 - val_loss: 16.3134 - val_accuracy: 0.0250\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.8669 - accuracy: 1.0000 - val_loss: 17.1343 - val_accuracy: 0.0250\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 104ms/step - loss: 0.8573 - accuracy: 0.9950 - val_loss: 15.8401 - val_accuracy: 0.0250\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.8957 - accuracy: 0.9775 - val_loss: 16.6865 - val_accuracy: 0.0250\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.8851 - accuracy: 0.9887 - val_loss: 16.4726 - val_accuracy: 0.0250\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.9408 - accuracy: 0.9625 - val_loss: 15.2215 - val_accuracy: 0.0250\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.9397 - accuracy: 0.9837 - val_loss: 16.7791 - val_accuracy: 0.0250\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.8990 - accuracy: 0.9588 - val_loss: 15.4966 - val_accuracy: 0.0250\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.8891 - accuracy: 0.9663 - val_loss: 16.2481 - val_accuracy: 0.0250\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.8369 - accuracy: 0.9950 - val_loss: 16.5512 - val_accuracy: 0.0250\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.8426 - accuracy: 0.9987 - val_loss: 16.6711 - val_accuracy: 0.0250\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.9017 - accuracy: 0.9825 - val_loss: 16.3865 - val_accuracy: 0.0250\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.9782 - accuracy: 0.7775 - val_loss: 16.7032 - val_accuracy: 0.0250\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 1.0055 - accuracy: 0.7450 - val_loss: 15.7977 - val_accuracy: 0.0250\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.9689 - accuracy: 0.8438 - val_loss: 16.6145 - val_accuracy: 0.0250\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.8866 - accuracy: 0.9837 - val_loss: 16.0640 - val_accuracy: 0.0250\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.8229 - accuracy: 0.9937 - val_loss: 16.4695 - val_accuracy: 0.0250\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.8317 - accuracy: 0.9937 - val_loss: 15.7792 - val_accuracy: 0.0250\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.8209 - accuracy: 0.9962 - val_loss: 16.2874 - val_accuracy: 0.0250\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.8254 - accuracy: 0.9937 - val_loss: 15.6286 - val_accuracy: 0.0250\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.7845 - accuracy: 0.9987 - val_loss: 16.7771 - val_accuracy: 0.0250\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.7449 - accuracy: 0.9787 - val_loss: 15.9068 - val_accuracy: 0.0250\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.7507 - accuracy: 0.9725 - val_loss: 16.6639 - val_accuracy: 0.0250\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.7095 - accuracy: 0.9950 - val_loss: 16.7251 - val_accuracy: 0.0250\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.7105 - accuracy: 1.0000 - val_loss: 16.6344 - val_accuracy: 0.0250\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.7385 - accuracy: 1.0000 - val_loss: 16.2879 - val_accuracy: 0.0250\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.7886 - accuracy: 0.9837 - val_loss: 16.5475 - val_accuracy: 0.0250\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.9282 - accuracy: 0.8012 - val_loss: 15.0560 - val_accuracy: 0.0250\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.0349 - accuracy: 0.6612 - val_loss: 16.0037 - val_accuracy: 0.0250\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.9727 - accuracy: 0.7763 - val_loss: 15.6496 - val_accuracy: 0.0250\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.8337 - accuracy: 0.9413 - val_loss: 16.0441 - val_accuracy: 0.0250\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.7258 - accuracy: 0.9962 - val_loss: 16.7099 - val_accuracy: 0.0250\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.6589 - accuracy: 1.0000 - val_loss: 16.2253 - val_accuracy: 0.0250\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.6415 - accuracy: 1.0000 - val_loss: 17.3822 - val_accuracy: 0.0250\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.6246 - accuracy: 1.0000 - val_loss: 15.9461 - val_accuracy: 0.0250\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.6383 - accuracy: 0.9987 - val_loss: 17.2759 - val_accuracy: 0.0250\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.6438 - accuracy: 1.0000 - val_loss: 16.0403 - val_accuracy: 0.0250\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.7268 - accuracy: 0.9837 - val_loss: 15.1221 - val_accuracy: 0.0250\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.7524 - accuracy: 0.9962 - val_loss: 16.6430 - val_accuracy: 0.0250\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.7298 - accuracy: 0.9900 - val_loss: 14.8453 - val_accuracy: 0.0250\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.6994 - accuracy: 0.9787 - val_loss: 16.2819 - val_accuracy: 0.0250\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.6374 - accuracy: 0.9987 - val_loss: 16.0938 - val_accuracy: 0.0250\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.6449 - accuracy: 1.0000 - val_loss: 16.7915 - val_accuracy: 0.0250\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.6697 - accuracy: 0.9962 - val_loss: 16.2930 - val_accuracy: 0.0250\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.6958 - accuracy: 0.9725 - val_loss: 16.9591 - val_accuracy: 0.0250\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.7495 - accuracy: 0.9312 - val_loss: 16.0645 - val_accuracy: 0.0250\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.7735 - accuracy: 0.9225 - val_loss: 16.4792 - val_accuracy: 0.0250\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.7378 - accuracy: 0.9750 - val_loss: 15.9920 - val_accuracy: 0.0250\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.6762 - accuracy: 0.9887 - val_loss: 15.9688 - val_accuracy: 0.0250\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.6589 - accuracy: 1.0000 - val_loss: 15.8810 - val_accuracy: 0.0250\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.6218 - accuracy: 1.0000 - val_loss: 15.6929 - val_accuracy: 0.0250\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.6013 - accuracy: 1.0000 - val_loss: 15.9966 - val_accuracy: 0.0250\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.5526 - accuracy: 1.0000 - val_loss: 16.5707 - val_accuracy: 0.0250\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.5310 - accuracy: 1.0000 - val_loss: 16.1972 - val_accuracy: 0.0250\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.5229 - accuracy: 1.0000 - val_loss: 17.1455 - val_accuracy: 0.0250\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.5284 - accuracy: 0.9937 - val_loss: 15.7497 - val_accuracy: 0.0250\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.5609 - accuracy: 0.9762 - val_loss: 16.5161 - val_accuracy: 0.0250\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.5167 - accuracy: 0.9962 - val_loss: 16.3849 - val_accuracy: 0.0250\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.5126 - accuracy: 1.0000 - val_loss: 16.4625 - val_accuracy: 0.0250\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.5350 - accuracy: 1.0000 - val_loss: 16.0892 - val_accuracy: 0.0250\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.5740 - accuracy: 1.0000 - val_loss: 16.5697 - val_accuracy: 0.0250\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.6718 - accuracy: 0.9425 - val_loss: 15.1508 - val_accuracy: 0.0250\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.7516 - accuracy: 0.8475 - val_loss: 16.0390 - val_accuracy: 0.0250\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.7091 - accuracy: 0.8550 - val_loss: 15.8663 - val_accuracy: 0.0250\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.6548 - accuracy: 0.9337 - val_loss: 16.2380 - val_accuracy: 0.0250\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.5295 - accuracy: 0.9987 - val_loss: 16.3927 - val_accuracy: 0.0250\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.4653 - accuracy: 1.0000 - val_loss: 16.5419 - val_accuracy: 0.0250\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.4384 - accuracy: 1.0000 - val_loss: 16.7973 - val_accuracy: 0.0250\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.4284 - accuracy: 1.0000 - val_loss: 16.5943 - val_accuracy: 0.0250\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.4295 - accuracy: 1.0000 - val_loss: 16.8967 - val_accuracy: 0.0250\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.4349 - accuracy: 1.0000 - val_loss: 16.3105 - val_accuracy: 0.0250\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.4726 - accuracy: 1.0000 - val_loss: 15.9943 - val_accuracy: 0.0250\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.5053 - accuracy: 1.0000 - val_loss: 16.3515 - val_accuracy: 0.0250\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.5701 - accuracy: 0.9937 - val_loss: 14.4631 - val_accuracy: 0.0250\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 150ms/step - loss: 0.5424 - accuracy: 0.9975 - val_loss: 16.2861 - val_accuracy: 0.0250\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.4716 - accuracy: 1.0000 - val_loss: 15.7761 - val_accuracy: 0.0250\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.4437 - accuracy: 1.0000 - val_loss: 16.4885 - val_accuracy: 0.0250\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.4349 - accuracy: 1.0000 - val_loss: 16.1642 - val_accuracy: 0.0250\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.4437 - accuracy: 1.0000 - val_loss: 16.9324 - val_accuracy: 0.0250\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.4469 - accuracy: 1.0000 - val_loss: 15.9162 - val_accuracy: 0.0250\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.4495 - accuracy: 1.0000 - val_loss: 17.1314 - val_accuracy: 0.0250\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.4433 - accuracy: 1.0000 - val_loss: 15.9031 - val_accuracy: 0.0250\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.4614 - accuracy: 1.0000 - val_loss: 16.6083 - val_accuracy: 0.0250\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.4376 - accuracy: 1.0000 - val_loss: 16.2205 - val_accuracy: 0.0250\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.4352 - accuracy: 1.0000 - val_loss: 15.6989 - val_accuracy: 0.0250\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.4574 - accuracy: 1.0000 - val_loss: 15.4949 - val_accuracy: 0.0250\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.4407 - accuracy: 1.0000 - val_loss: 15.7033 - val_accuracy: 0.0250\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.4318 - accuracy: 1.0000 - val_loss: 15.7201 - val_accuracy: 0.0250\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3839 - accuracy: 1.0000 - val_loss: 16.1516 - val_accuracy: 0.0250\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3523 - accuracy: 1.0000 - val_loss: 16.5413 - val_accuracy: 0.0250\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3394 - accuracy: 1.0000 - val_loss: 16.3337 - val_accuracy: 0.0250\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3341 - accuracy: 1.0000 - val_loss: 16.9674 - val_accuracy: 0.0250\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.3315 - accuracy: 1.0000 - val_loss: 16.3176 - val_accuracy: 0.0250\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3362 - accuracy: 1.0000 - val_loss: 17.1026 - val_accuracy: 0.0250\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3536 - accuracy: 1.0000 - val_loss: 16.1942 - val_accuracy: 0.0250\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.4088 - accuracy: 1.0000 - val_loss: 16.3472 - val_accuracy: 0.0250\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.4954 - accuracy: 0.9762 - val_loss: 16.6041 - val_accuracy: 0.0250\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.5217 - accuracy: 0.9563 - val_loss: 15.4319 - val_accuracy: 0.0250\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.4775 - accuracy: 0.9875 - val_loss: 16.3897 - val_accuracy: 0.0250\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.4255 - accuracy: 0.9950 - val_loss: 15.4694 - val_accuracy: 0.0250\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3548 - accuracy: 1.0000 - val_loss: 16.1962 - val_accuracy: 0.0250\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.3315 - accuracy: 1.0000 - val_loss: 15.6472 - val_accuracy: 0.0250\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.3345 - accuracy: 0.9975 - val_loss: 15.9250 - val_accuracy: 0.0250\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.3088 - accuracy: 1.0000 - val_loss: 15.7505 - val_accuracy: 0.0250\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.2856 - accuracy: 1.0000 - val_loss: 16.3591 - val_accuracy: 0.0250\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.2741 - accuracy: 1.0000 - val_loss: 16.3935 - val_accuracy: 0.0250\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.2682 - accuracy: 1.0000 - val_loss: 16.4373 - val_accuracy: 0.0250\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.2679 - accuracy: 1.0000 - val_loss: 16.6324 - val_accuracy: 0.0250\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.2692 - accuracy: 1.0000 - val_loss: 16.2362 - val_accuracy: 0.0250\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.2832 - accuracy: 1.0000 - val_loss: 16.3976 - val_accuracy: 0.0250\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.2944 - accuracy: 1.0000 - val_loss: 15.9993 - val_accuracy: 0.0250\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.3246 - accuracy: 1.0000 - val_loss: 15.5676 - val_accuracy: 0.0250\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3241 - accuracy: 1.0000 - val_loss: 16.2877 - val_accuracy: 0.0250\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.3092 - accuracy: 1.0000 - val_loss: 15.7434 - val_accuracy: 0.0250\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.2925 - accuracy: 1.0000 - val_loss: 16.5387 - val_accuracy: 0.0250\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.2918 - accuracy: 1.0000 - val_loss: 15.9965 - val_accuracy: 0.0250\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3278 - accuracy: 0.9987 - val_loss: 16.7447 - val_accuracy: 0.0250\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.3439 - accuracy: 0.9875 - val_loss: 15.9143 - val_accuracy: 0.0250\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.3274 - accuracy: 0.9962 - val_loss: 16.7579 - val_accuracy: 0.0250\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.3023 - accuracy: 1.0000 - val_loss: 15.8425 - val_accuracy: 0.0250\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.2827 - accuracy: 0.9962 - val_loss: 16.6006 - val_accuracy: 0.0250\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.2425 - accuracy: 1.0000 - val_loss: 16.1257 - val_accuracy: 0.0250\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.2235 - accuracy: 1.0000 - val_loss: 16.4226 - val_accuracy: 0.0250\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.2232 - accuracy: 1.0000 - val_loss: 16.2181 - val_accuracy: 0.0250\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.2247 - accuracy: 1.0000 - val_loss: 16.2325 - val_accuracy: 0.0250\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.2339 - accuracy: 1.0000 - val_loss: 15.8827 - val_accuracy: 0.0250\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.2305 - accuracy: 1.0000 - val_loss: 16.3390 - val_accuracy: 0.0250\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.2311 - accuracy: 1.0000 - val_loss: 15.7699 - val_accuracy: 0.0250\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.2269 - accuracy: 1.0000 - val_loss: 16.4213 - val_accuracy: 0.0250\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.2284 - accuracy: 1.0000 - val_loss: 15.9827 - val_accuracy: 0.0250\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.2344 - accuracy: 1.0000 - val_loss: 16.2747 - val_accuracy: 0.0250\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.2433 - accuracy: 1.0000 - val_loss: 16.1731 - val_accuracy: 0.0250\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.2541 - accuracy: 1.0000 - val_loss: 16.0663 - val_accuracy: 0.0250\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.2526 - accuracy: 1.0000 - val_loss: 16.2320 - val_accuracy: 0.0250\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.2432 - accuracy: 1.0000 - val_loss: 16.1324 - val_accuracy: 0.0250\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.2228 - accuracy: 1.0000 - val_loss: 16.3794 - val_accuracy: 0.0250\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.1978 - accuracy: 1.0000 - val_loss: 16.3966 - val_accuracy: 0.0250\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.1757 - accuracy: 1.0000 - val_loss: 16.6641 - val_accuracy: 0.0250\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.1654 - accuracy: 1.0000 - val_loss: 16.4560 - val_accuracy: 0.0250\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.1639 - accuracy: 1.0000 - val_loss: 16.7277 - val_accuracy: 0.0250\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.1646 - accuracy: 1.0000 - val_loss: 16.2683 - val_accuracy: 0.0250\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.1749 - accuracy: 1.0000 - val_loss: 16.2296 - val_accuracy: 0.0250\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.1797 - accuracy: 1.0000 - val_loss: 16.5537 - val_accuracy: 0.0250\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.1970 - accuracy: 1.0000 - val_loss: 15.2345 - val_accuracy: 0.0100\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.2109 - accuracy: 1.0000 - val_loss: 16.5119 - val_accuracy: 0.0250\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.1994 - accuracy: 1.0000 - val_loss: 15.2937 - val_accuracy: 0.0250\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.1871 - accuracy: 1.0000 - val_loss: 16.3812 - val_accuracy: 0.0250\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.1799 - accuracy: 1.0000 - val_loss: 15.6175 - val_accuracy: 0.0250\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.1861 - accuracy: 1.0000 - val_loss: 16.4919 - val_accuracy: 0.0250\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.2021 - accuracy: 1.0000 - val_loss: 15.5777 - val_accuracy: 0.0250\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.2453 - accuracy: 0.9987 - val_loss: 16.7505 - val_accuracy: 0.0250\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.2662 - accuracy: 0.9900 - val_loss: 15.6823 - val_accuracy: 0.0250\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.3415 - accuracy: 0.9638 - val_loss: 16.2707 - val_accuracy: 0.0250\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.2544 - accuracy: 0.9987 - val_loss: 15.9514 - val_accuracy: 0.0250\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.1663 - accuracy: 1.0000 - val_loss: 15.9757 - val_accuracy: 0.0250\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.1375 - accuracy: 1.0000 - val_loss: 16.3722 - val_accuracy: 0.0250\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.1293 - accuracy: 1.0000 - val_loss: 15.9835 - val_accuracy: 0.0250\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.1257 - accuracy: 1.0000 - val_loss: 16.6103 - val_accuracy: 0.0250\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.1217 - accuracy: 1.0000 - val_loss: 16.1140 - val_accuracy: 0.0250\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.1192 - accuracy: 1.0000 - val_loss: 16.7631 - val_accuracy: 0.0250\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.1165 - accuracy: 1.0000 - val_loss: 16.1796 - val_accuracy: 0.0250\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.1158 - accuracy: 1.0000 - val_loss: 16.8299 - val_accuracy: 0.0250\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.1144 - accuracy: 1.0000 - val_loss: 16.1960 - val_accuracy: 0.0250\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.1173 - accuracy: 1.0000 - val_loss: 16.6826 - val_accuracy: 0.0250\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.1195 - accuracy: 1.0000 - val_loss: 16.2054 - val_accuracy: 0.0250\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.1345 - accuracy: 1.0000 - val_loss: 15.9720 - val_accuracy: 0.0250\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.1499 - accuracy: 1.0000 - val_loss: 16.2373 - val_accuracy: 0.0250\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.1795 - accuracy: 1.0000 - val_loss: 14.9101 - val_accuracy: 0.0250\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.1833 - accuracy: 1.0000 - val_loss: 16.0024 - val_accuracy: 0.0250\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.1468 - accuracy: 1.0000 - val_loss: 15.8253 - val_accuracy: 0.0250\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.1160 - accuracy: 1.0000 - val_loss: 16.0487 - val_accuracy: 0.0250\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.1000 - accuracy: 1.0000 - val_loss: 16.2892 - val_accuracy: 0.0250\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0945 - accuracy: 1.0000 - val_loss: 16.3705 - val_accuracy: 0.0250\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0917 - accuracy: 1.0000 - val_loss: 16.5005 - val_accuracy: 0.0250\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0896 - accuracy: 1.0000 - val_loss: 16.5779 - val_accuracy: 0.0250\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0878 - accuracy: 1.0000 - val_loss: 16.6458 - val_accuracy: 0.0250\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0861 - accuracy: 1.0000 - val_loss: 16.7219 - val_accuracy: 0.0250\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0845 - accuracy: 1.0000 - val_loss: 16.7318 - val_accuracy: 0.0250\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0832 - accuracy: 1.0000 - val_loss: 16.8393 - val_accuracy: 0.0250\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0824 - accuracy: 1.0000 - val_loss: 16.6834 - val_accuracy: 0.0250\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.0822 - accuracy: 1.0000 - val_loss: 17.0072 - val_accuracy: 0.0250\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0835 - accuracy: 1.0000 - val_loss: 16.2741 - val_accuracy: 0.0250\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0946 - accuracy: 1.0000 - val_loss: 17.3053 - val_accuracy: 0.0250\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.1527 - accuracy: 0.9750 - val_loss: 15.1815 - val_accuracy: 0.0000e+00\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.2021 - accuracy: 0.9787 - val_loss: 15.2224 - val_accuracy: 0.0200\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.1219 - accuracy: 0.9950 - val_loss: 15.2815 - val_accuracy: 0.0250\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.1314 - accuracy: 1.0000 - val_loss: 15.4471 - val_accuracy: 0.0250\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.2228 - accuracy: 0.9887 - val_loss: 14.8860 - val_accuracy: 0.0250\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.3594 - accuracy: 0.9312 - val_loss: 15.2041 - val_accuracy: 0.0250\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.1765 - accuracy: 0.9987 - val_loss: 15.0663 - val_accuracy: 0.0250\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0886 - accuracy: 1.0000 - val_loss: 15.2711 - val_accuracy: 0.0250\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0751 - accuracy: 1.0000 - val_loss: 15.4805 - val_accuracy: 0.0250\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0721 - accuracy: 1.0000 - val_loss: 15.6475 - val_accuracy: 0.0250\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0697 - accuracy: 1.0000 - val_loss: 15.7902 - val_accuracy: 0.0250\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0678 - accuracy: 1.0000 - val_loss: 15.9099 - val_accuracy: 0.0250\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0661 - accuracy: 1.0000 - val_loss: 16.0171 - val_accuracy: 0.0250\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0646 - accuracy: 1.0000 - val_loss: 16.1117 - val_accuracy: 0.0250\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0632 - accuracy: 1.0000 - val_loss: 16.1976 - val_accuracy: 0.0250\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0618 - accuracy: 1.0000 - val_loss: 16.2743 - val_accuracy: 0.0250\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0606 - accuracy: 1.0000 - val_loss: 16.3446 - val_accuracy: 0.0250\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0593 - accuracy: 1.0000 - val_loss: 16.4078 - val_accuracy: 0.0250\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0581 - accuracy: 1.0000 - val_loss: 16.4660 - val_accuracy: 0.0250\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0570 - accuracy: 1.0000 - val_loss: 16.5183 - val_accuracy: 0.0250\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0558 - accuracy: 1.0000 - val_loss: 16.5670 - val_accuracy: 0.0250\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0547 - accuracy: 1.0000 - val_loss: 16.6110 - val_accuracy: 0.0250\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.0536 - accuracy: 1.0000 - val_loss: 16.6529 - val_accuracy: 0.0250\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0526 - accuracy: 1.0000 - val_loss: 16.6908 - val_accuracy: 0.0250\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0515 - accuracy: 1.0000 - val_loss: 16.7284 - val_accuracy: 0.0250\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 16.7611 - val_accuracy: 0.0250\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0494 - accuracy: 1.0000 - val_loss: 16.7967 - val_accuracy: 0.0250\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0484 - accuracy: 1.0000 - val_loss: 16.8228 - val_accuracy: 0.0250\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0474 - accuracy: 1.0000 - val_loss: 16.8624 - val_accuracy: 0.0250\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 16.8691 - val_accuracy: 0.0250\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0455 - accuracy: 1.0000 - val_loss: 16.9402 - val_accuracy: 0.0250\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0448 - accuracy: 1.0000 - val_loss: 16.8427 - val_accuracy: 0.0250\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0459 - accuracy: 1.0000 - val_loss: 17.0914 - val_accuracy: 0.0250\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0590 - accuracy: 1.0000 - val_loss: 15.8189 - val_accuracy: 0.0250\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.1739 - accuracy: 0.9987 - val_loss: 14.4318 - val_accuracy: 0.0000e+00\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.4941 - accuracy: 0.8375 - val_loss: 14.5340 - val_accuracy: 0.0250\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.2617 - accuracy: 0.9837 - val_loss: 13.7512 - val_accuracy: 0.0250\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0987 - accuracy: 1.0000 - val_loss: 14.4292 - val_accuracy: 0.0250\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0567 - accuracy: 1.0000 - val_loss: 14.6797 - val_accuracy: 0.0250\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0517 - accuracy: 1.0000 - val_loss: 14.9002 - val_accuracy: 0.0250\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0491 - accuracy: 1.0000 - val_loss: 15.0897 - val_accuracy: 0.0250\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0471 - accuracy: 1.0000 - val_loss: 15.2506 - val_accuracy: 0.0250\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0454 - accuracy: 1.0000 - val_loss: 15.3876 - val_accuracy: 0.0250\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0439 - accuracy: 1.0000 - val_loss: 15.5058 - val_accuracy: 0.0250\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0426 - accuracy: 1.0000 - val_loss: 15.6097 - val_accuracy: 0.0250\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0414 - accuracy: 1.0000 - val_loss: 15.7031 - val_accuracy: 0.0250\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0403 - accuracy: 1.0000 - val_loss: 15.7885 - val_accuracy: 0.0250\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0393 - accuracy: 1.0000 - val_loss: 15.8679 - val_accuracy: 0.0250\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0384 - accuracy: 1.0000 - val_loss: 15.9424 - val_accuracy: 0.0250\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 16.0126 - val_accuracy: 0.0250\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0367 - accuracy: 1.0000 - val_loss: 16.0792 - val_accuracy: 0.0250\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0359 - accuracy: 1.0000 - val_loss: 16.1422 - val_accuracy: 0.0250\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 16.2020 - val_accuracy: 0.0250\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0344 - accuracy: 1.0000 - val_loss: 16.2587 - val_accuracy: 0.0250\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0337 - accuracy: 1.0000 - val_loss: 16.3125 - val_accuracy: 0.0250\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 16.3634 - val_accuracy: 0.0250\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0324 - accuracy: 1.0000 - val_loss: 16.4116 - val_accuracy: 0.0250\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 16.4573 - val_accuracy: 0.0250\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0312 - accuracy: 1.0000 - val_loss: 16.5006 - val_accuracy: 0.0250\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 16.5417 - val_accuracy: 0.0250\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 16.5807 - val_accuracy: 0.0250\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 16.6178 - val_accuracy: 0.0250\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 16.6530 - val_accuracy: 0.0250\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0283 - accuracy: 1.0000 - val_loss: 16.6868 - val_accuracy: 0.0250\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0278 - accuracy: 1.0000 - val_loss: 16.7188 - val_accuracy: 0.0250\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0272 - accuracy: 1.0000 - val_loss: 16.7500 - val_accuracy: 0.0250\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 16.7789 - val_accuracy: 0.0250\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 16.8087 - val_accuracy: 0.0250\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 16.8337 - val_accuracy: 0.0250\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 16.8649 - val_accuracy: 0.0250\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 16.8814 - val_accuracy: 0.0250\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 16.9238 - val_accuracy: 0.0250\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 16.9143 - val_accuracy: 0.0250\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0230 - accuracy: 1.0000 - val_loss: 17.0015 - val_accuracy: 0.0250\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 16.9016 - val_accuracy: 0.0250\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 17.1402 - val_accuracy: 0.0250\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0219 - accuracy: 1.0000 - val_loss: 16.7804 - val_accuracy: 0.0250\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 17.2091 - val_accuracy: 0.0250\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 16.7043 - val_accuracy: 0.0250\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 17.0357 - val_accuracy: 0.0250\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.2570 - accuracy: 0.9038 - val_loss: 16.2704 - val_accuracy: 0.0250\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.3849 - accuracy: 0.8500 - val_loss: 16.5607 - val_accuracy: 0.0250\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.2628 - accuracy: 0.9500 - val_loss: 15.3451 - val_accuracy: 0.0100\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.1060 - accuracy: 1.0000 - val_loss: 15.2617 - val_accuracy: 0.0250\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 15.3945 - val_accuracy: 0.0250\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 15.5172 - val_accuracy: 0.0250\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0266 - accuracy: 1.0000 - val_loss: 15.6214 - val_accuracy: 0.0250\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0254 - accuracy: 1.0000 - val_loss: 15.7103 - val_accuracy: 0.0250\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 15.7879 - val_accuracy: 0.0250\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 15.8570 - val_accuracy: 0.0250\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 15.9196 - val_accuracy: 0.0250\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 15.9772 - val_accuracy: 0.0250\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0212 - accuracy: 1.0000 - val_loss: 16.0322 - val_accuracy: 0.0250\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0206 - accuracy: 1.0000 - val_loss: 16.0850 - val_accuracy: 0.0250\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0201 - accuracy: 1.0000 - val_loss: 16.1352 - val_accuracy: 0.0250\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0196 - accuracy: 1.0000 - val_loss: 16.1831 - val_accuracy: 0.0250\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 16.2289 - val_accuracy: 0.0250\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0188 - accuracy: 1.0000 - val_loss: 16.2728 - val_accuracy: 0.0250\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 16.3151 - val_accuracy: 0.0250\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 16.3557 - val_accuracy: 0.0250\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 105ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 16.3948 - val_accuracy: 0.0250\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 16.4322 - val_accuracy: 0.0250\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 16.4680 - val_accuracy: 0.0250\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0165 - accuracy: 1.0000 - val_loss: 16.5022 - val_accuracy: 0.0250\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 16.5349 - val_accuracy: 0.0250\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 16.5662 - val_accuracy: 0.0250\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 16.5961 - val_accuracy: 0.0250\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 16.6249 - val_accuracy: 0.0250\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0150 - accuracy: 1.0000 - val_loss: 16.6527 - val_accuracy: 0.0250\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0147 - accuracy: 1.0000 - val_loss: 16.6795 - val_accuracy: 0.0250\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 16.7055 - val_accuracy: 0.0250\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 16.7308 - val_accuracy: 0.0250\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 16.7553 - val_accuracy: 0.0250\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 16.7792 - val_accuracy: 0.0250\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 16.8025 - val_accuracy: 0.0250\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 16.8252 - val_accuracy: 0.0250\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 16.8474 - val_accuracy: 0.0250\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 16.8690 - val_accuracy: 0.0250\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 16.8902 - val_accuracy: 0.0250\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 16.9109 - val_accuracy: 0.0250\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 16.9313 - val_accuracy: 0.0250\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 16.9512 - val_accuracy: 0.0250\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 16.9708 - val_accuracy: 0.0250\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0111 - accuracy: 1.0000 - val_loss: 16.9900 - val_accuracy: 0.0250\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 17.0090 - val_accuracy: 0.0250\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0107 - accuracy: 1.0000 - val_loss: 17.0277 - val_accuracy: 0.0250\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 17.0462 - val_accuracy: 0.0250\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 17.0645 - val_accuracy: 0.0250\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 17.0826 - val_accuracy: 0.0250\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 17.1007 - val_accuracy: 0.0250\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 17.1183 - val_accuracy: 0.0250\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 17.1358 - val_accuracy: 0.0250\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 17.1527 - val_accuracy: 0.0250\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0089 - accuracy: 1.0000 - val_loss: 17.1633 - val_accuracy: 0.0250\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 17.1792 - val_accuracy: 0.0250\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 17.0617 - val_accuracy: 0.0250\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 17.2456 - val_accuracy: 0.0250\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0125 - accuracy: 1.0000 - val_loss: 16.3672 - val_accuracy: 0.0250\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0361 - accuracy: 1.0000 - val_loss: 15.4253 - val_accuracy: 0.0000e+00\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.2990 - accuracy: 0.9013 - val_loss: 14.6717 - val_accuracy: 0.0250\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.1551 - accuracy: 0.9925 - val_loss: 13.7139 - val_accuracy: 0.0250\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0436 - accuracy: 1.0000 - val_loss: 14.2478 - val_accuracy: 0.0250\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 14.3730 - val_accuracy: 0.0250\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0154 - accuracy: 1.0000 - val_loss: 14.4916 - val_accuracy: 0.0250\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 14.5984 - val_accuracy: 0.0250\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 14.6968 - val_accuracy: 0.0250\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 14.7891 - val_accuracy: 0.0250\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0120 - accuracy: 1.0000 - val_loss: 14.8760 - val_accuracy: 0.0250\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0115 - accuracy: 1.0000 - val_loss: 14.9590 - val_accuracy: 0.0250\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 15.0394 - val_accuracy: 0.0250\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 15.1177 - val_accuracy: 0.0250\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0103 - accuracy: 1.0000 - val_loss: 15.1941 - val_accuracy: 0.0250\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 15.2688 - val_accuracy: 0.0250\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 15.3419 - val_accuracy: 0.0250\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 15.4138 - val_accuracy: 0.0250\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 15.4845 - val_accuracy: 0.0250\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 15.5544 - val_accuracy: 0.0250\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 15.6241 - val_accuracy: 0.0250\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0083 - accuracy: 1.0000 - val_loss: 15.6945 - val_accuracy: 0.0250\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 15.7647 - val_accuracy: 0.0250\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 15.8319 - val_accuracy: 0.0250\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 15.8963 - val_accuracy: 0.0250\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 15.9581 - val_accuracy: 0.0250\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 16.0177 - val_accuracy: 0.0250\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 16.0750 - val_accuracy: 0.0250\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 16.1303 - val_accuracy: 0.0250\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 16.1834 - val_accuracy: 0.0250\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 16.2346 - val_accuracy: 0.0250\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 16.2837 - val_accuracy: 0.0250\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 107ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 16.3310 - val_accuracy: 0.0250\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 16.3766 - val_accuracy: 0.0250\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 16.4205 - val_accuracy: 0.0250\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0060 - accuracy: 1.0000 - val_loss: 16.4628 - val_accuracy: 0.0250\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 16.5036 - val_accuracy: 0.0250\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 16.5430 - val_accuracy: 0.0250\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 16.5810 - val_accuracy: 0.0250\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 16.6175 - val_accuracy: 0.0250\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 16.6526 - val_accuracy: 0.0250\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 16.6864 - val_accuracy: 0.0250\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0053 - accuracy: 1.0000 - val_loss: 16.7189 - val_accuracy: 0.0250\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 16.7502 - val_accuracy: 0.0250\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 16.7802 - val_accuracy: 0.0250\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 16.8092 - val_accuracy: 0.0250\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 16.8371 - val_accuracy: 0.0250\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 16.8641 - val_accuracy: 0.0250\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 16.8901 - val_accuracy: 0.0250\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 16.9153 - val_accuracy: 0.0250\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 16.9397 - val_accuracy: 0.0250\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 167ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 16.9633 - val_accuracy: 0.0250\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 16.9863 - val_accuracy: 0.0250\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 17.0086 - val_accuracy: 0.0250\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 17.0303 - val_accuracy: 0.0250\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 17.0515 - val_accuracy: 0.0250\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 17.0722 - val_accuracy: 0.0250\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 17.0924 - val_accuracy: 0.0250\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 17.1122 - val_accuracy: 0.0250\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 17.1317 - val_accuracy: 0.0250\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 17.1505 - val_accuracy: 0.0250\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 17.1698 - val_accuracy: 0.0250\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 17.1866 - val_accuracy: 0.0250\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 17.2093 - val_accuracy: 0.0250\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 17.2143 - val_accuracy: 0.0250\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 17.2555 - val_accuracy: 0.0250\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 17.1648 - val_accuracy: 0.0250\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 17.5149 - val_accuracy: 0.0250\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.1142 - accuracy: 0.9725 - val_loss: 16.5499 - val_accuracy: 0.0250\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.2710 - accuracy: 0.8888 - val_loss: 17.0411 - val_accuracy: 0.0250\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.2087 - accuracy: 0.9475 - val_loss: 15.5972 - val_accuracy: 0.0250\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0513 - accuracy: 0.9987 - val_loss: 15.5018 - val_accuracy: 0.0250\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 15.5722 - val_accuracy: 0.0250\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 15.6313 - val_accuracy: 0.0250\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 15.6858 - val_accuracy: 0.0250\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 15.7365 - val_accuracy: 0.0250\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0065 - accuracy: 1.0000 - val_loss: 15.7812 - val_accuracy: 0.0250\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 15.8215 - val_accuracy: 0.0250\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 15.8595 - val_accuracy: 0.0250\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 15.8960 - val_accuracy: 0.0250\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 15.9313 - val_accuracy: 0.0250\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 15.9656 - val_accuracy: 0.0250\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 15.9986 - val_accuracy: 0.0250\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 16.0307 - val_accuracy: 0.0250\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 16.0619 - val_accuracy: 0.0250\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 16.0923 - val_accuracy: 0.0250\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 16.1221 - val_accuracy: 0.0250\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 16.1513 - val_accuracy: 0.0250\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 16.1800 - val_accuracy: 0.0250\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 16.2082 - val_accuracy: 0.0250\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 16.2359 - val_accuracy: 0.0250\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 16.2632 - val_accuracy: 0.0250\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 16.2900 - val_accuracy: 0.0250\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 16.3164 - val_accuracy: 0.0250\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 16.3424 - val_accuracy: 0.0250\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 16.3681 - val_accuracy: 0.0250\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 16.3933 - val_accuracy: 0.0250\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 16.4182 - val_accuracy: 0.0250\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 16.4427 - val_accuracy: 0.0250\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 16.4669 - val_accuracy: 0.0250\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 16.4908 - val_accuracy: 0.0250\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 16.5144 - val_accuracy: 0.0250\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 16.5377 - val_accuracy: 0.0250\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 16.5607 - val_accuracy: 0.0250\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 16.5834 - val_accuracy: 0.0250\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 16.6058 - val_accuracy: 0.0250\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 16.6280 - val_accuracy: 0.0250\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 16.6499 - val_accuracy: 0.0250\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 16.6716 - val_accuracy: 0.0250\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 16.6930 - val_accuracy: 0.0250\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 16.7141 - val_accuracy: 0.0250\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 16.7351 - val_accuracy: 0.0250\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 16.7558 - val_accuracy: 0.0250\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 16.7763 - val_accuracy: 0.0250\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 16.7965 - val_accuracy: 0.0250\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 16.8165 - val_accuracy: 0.0250\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 16.8363 - val_accuracy: 0.0250\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 16.8559 - val_accuracy: 0.0250\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 16.8752 - val_accuracy: 0.0250\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 16.8943 - val_accuracy: 0.0250\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 16.9132 - val_accuracy: 0.0250\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 16.9319 - val_accuracy: 0.0250\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 16.9503 - val_accuracy: 0.0250\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 16.9686 - val_accuracy: 0.0250\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 16.9867 - val_accuracy: 0.0250\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 17.0046 - val_accuracy: 0.0250\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 17.0224 - val_accuracy: 0.0250\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 17.0401 - val_accuracy: 0.0250\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 17.0581 - val_accuracy: 0.0250\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 17.0764 - val_accuracy: 0.0250\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 17.0950 - val_accuracy: 0.0250\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 17.1142 - val_accuracy: 0.0250\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 17.1331 - val_accuracy: 0.0250\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 17.1517 - val_accuracy: 0.0250\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 17.1692 - val_accuracy: 0.0250\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 17.1863 - val_accuracy: 0.0250\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 17.2021 - val_accuracy: 0.0250\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 17.2179 - val_accuracy: 0.0250\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 137ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 17.2194 - val_accuracy: 0.0250\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 17.2168 - val_accuracy: 0.0250\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 17.3050 - val_accuracy: 0.0250\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 17.2311 - val_accuracy: 0.0250\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 17.2482 - val_accuracy: 0.0250\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 17.2679 - val_accuracy: 0.0250\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 17.2368 - val_accuracy: 0.0250\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 17.2257 - val_accuracy: 0.0250\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 17.2599 - val_accuracy: 0.0250\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 17.0090 - val_accuracy: 0.0250\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 17.2190 - val_accuracy: 0.0250\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0857 - accuracy: 0.9750 - val_loss: 15.2883 - val_accuracy: 0.0100\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.2961 - accuracy: 0.8925 - val_loss: 15.6475 - val_accuracy: 0.0250\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0199 - accuracy: 1.0000 - val_loss: 15.5614 - val_accuracy: 0.0250\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 15.5427 - val_accuracy: 0.0250\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 15.5417 - val_accuracy: 0.0250\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 15.5463 - val_accuracy: 0.0250\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 15.5549 - val_accuracy: 0.0250\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 15.5664 - val_accuracy: 0.0250\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 15.5801 - val_accuracy: 0.0250\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 15.5960 - val_accuracy: 0.0250\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 15.6139 - val_accuracy: 0.0250\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 15.6334 - val_accuracy: 0.0250\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 15.6541 - val_accuracy: 0.0250\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 15.6758 - val_accuracy: 0.0250\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 15.6982 - val_accuracy: 0.0250\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 15.7216 - val_accuracy: 0.0250\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 15.7461 - val_accuracy: 0.0250\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 15.7715 - val_accuracy: 0.0250\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 15.7976 - val_accuracy: 0.0250\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 15.8243 - val_accuracy: 0.0250\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 15.8515 - val_accuracy: 0.0250\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 109ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 15.8791 - val_accuracy: 0.0250\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 15.9071 - val_accuracy: 0.0250\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 15.9355 - val_accuracy: 0.0250\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 15.9642 - val_accuracy: 0.0250\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 15.9932 - val_accuracy: 0.0250\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 16.0225 - val_accuracy: 0.0250\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 16.0520 - val_accuracy: 0.0250\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 16.0817 - val_accuracy: 0.0250\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 16.1117 - val_accuracy: 0.0250\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 16.1418 - val_accuracy: 0.0250\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 16.1721 - val_accuracy: 0.0250\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 16.2025 - val_accuracy: 0.0250\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 16.2329 - val_accuracy: 0.0250\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 16.2635 - val_accuracy: 0.0250\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 16.2941 - val_accuracy: 0.0250\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 16.3247 - val_accuracy: 0.0250\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 16.3553 - val_accuracy: 0.0250\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 16.3858 - val_accuracy: 0.0250\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 16.4163 - val_accuracy: 0.0250\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 16.4466 - val_accuracy: 0.0250\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 16.4768 - val_accuracy: 0.0250\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 16.5069 - val_accuracy: 0.0250\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 16.5367 - val_accuracy: 0.0250\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 16.5663 - val_accuracy: 0.0250\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 16.5956 - val_accuracy: 0.0250\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 16.6247 - val_accuracy: 0.0250\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 16.6534 - val_accuracy: 0.0250\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 16.6819 - val_accuracy: 0.0250\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 16.7101 - val_accuracy: 0.0250\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 16.7379 - val_accuracy: 0.0250\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 16.7653 - val_accuracy: 0.0250\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 16.7924 - val_accuracy: 0.0250\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 16.8191 - val_accuracy: 0.0250\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 16.8454 - val_accuracy: 0.0250\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 9.9400e-04 - accuracy: 1.0000 - val_loss: 16.8712 - val_accuracy: 0.0250\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 9.7717e-04 - accuracy: 1.0000 - val_loss: 16.8965 - val_accuracy: 0.0250\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 9.6065e-04 - accuracy: 1.0000 - val_loss: 16.9214 - val_accuracy: 0.0250\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 9.4442e-04 - accuracy: 1.0000 - val_loss: 16.9458 - val_accuracy: 0.0250\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 9.2846e-04 - accuracy: 1.0000 - val_loss: 16.9697 - val_accuracy: 0.0250\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 9.1276e-04 - accuracy: 1.0000 - val_loss: 16.9930 - val_accuracy: 0.0250\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 8.9731e-04 - accuracy: 1.0000 - val_loss: 17.0159 - val_accuracy: 0.0250\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 8.8210e-04 - accuracy: 1.0000 - val_loss: 17.0382 - val_accuracy: 0.0250\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 8.6713e-04 - accuracy: 1.0000 - val_loss: 17.0597 - val_accuracy: 0.0250\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 8.5232e-04 - accuracy: 1.0000 - val_loss: 17.0815 - val_accuracy: 0.0250\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 8.3777e-04 - accuracy: 1.0000 - val_loss: 17.0998 - val_accuracy: 0.0250\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 8.2346e-04 - accuracy: 1.0000 - val_loss: 17.1244 - val_accuracy: 0.0250\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 8.0976e-04 - accuracy: 1.0000 - val_loss: 17.1285 - val_accuracy: 0.0250\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 7.9956e-04 - accuracy: 1.0000 - val_loss: 17.1680 - val_accuracy: 0.0250\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 7.9618e-04 - accuracy: 1.0000 - val_loss: 17.1642 - val_accuracy: 0.0250\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 7.7744e-04 - accuracy: 1.0000 - val_loss: 17.1893 - val_accuracy: 0.0250\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 7.5536e-04 - accuracy: 1.0000 - val_loss: 17.2144 - val_accuracy: 0.0250\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 7.4119e-04 - accuracy: 1.0000 - val_loss: 17.2276 - val_accuracy: 0.0250\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 7.2772e-04 - accuracy: 1.0000 - val_loss: 17.2430 - val_accuracy: 0.0250\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 7.1450e-04 - accuracy: 1.0000 - val_loss: 17.2603 - val_accuracy: 0.0250\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 7.0150e-04 - accuracy: 1.0000 - val_loss: 17.2712 - val_accuracy: 0.0250\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 108ms/step - loss: 6.8869e-04 - accuracy: 1.0000 - val_loss: 17.2912 - val_accuracy: 0.0250\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 6.7613e-04 - accuracy: 1.0000 - val_loss: 17.2966 - val_accuracy: 0.0250\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 6.6386e-04 - accuracy: 1.0000 - val_loss: 17.3213 - val_accuracy: 0.0250\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 6.5207e-04 - accuracy: 1.0000 - val_loss: 17.3172 - val_accuracy: 0.0250\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 6.4095e-04 - accuracy: 1.0000 - val_loss: 17.3515 - val_accuracy: 0.0250\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 6.3122e-04 - accuracy: 1.0000 - val_loss: 17.3299 - val_accuracy: 0.0250\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 6.2260e-04 - accuracy: 1.0000 - val_loss: 17.3852 - val_accuracy: 0.0250\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 6.1762e-04 - accuracy: 1.0000 - val_loss: 17.3301 - val_accuracy: 0.0250\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 6.1509e-04 - accuracy: 1.0000 - val_loss: 17.4316 - val_accuracy: 0.0250\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 6.2209e-04 - accuracy: 1.0000 - val_loss: 17.3088 - val_accuracy: 0.0250\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 6.4530e-04 - accuracy: 1.0000 - val_loss: 17.4999 - val_accuracy: 0.0250\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 7.1785e-04 - accuracy: 1.0000 - val_loss: 17.2605 - val_accuracy: 0.0250\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 17.6594 - val_accuracy: 0.0250\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0071 - accuracy: 1.0000 - val_loss: 16.5659 - val_accuracy: 0.0250\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.3257 - accuracy: 0.8637 - val_loss: 17.8771 - val_accuracy: 0.0250\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0965 - accuracy: 0.9862 - val_loss: 16.4133 - val_accuracy: 0.0250\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0080 - accuracy: 1.0000 - val_loss: 16.1266 - val_accuracy: 0.0250\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 16.1273 - val_accuracy: 0.0250\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 16.1419 - val_accuracy: 0.0250\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 16.1624 - val_accuracy: 0.0250\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 16.1855 - val_accuracy: 0.0250\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 16.2102 - val_accuracy: 0.0250\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 16.2460 - val_accuracy: 0.0250\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 16.2654 - val_accuracy: 0.0250\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 16.2858 - val_accuracy: 0.0250\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 16.3063 - val_accuracy: 0.0250\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 16.3264 - val_accuracy: 0.0250\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 16.3460 - val_accuracy: 0.0250\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 16.3651 - val_accuracy: 0.0250\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 16.3836 - val_accuracy: 0.0250\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 16.4017 - val_accuracy: 0.0250\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 16.4193 - val_accuracy: 0.0250\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 16.4367 - val_accuracy: 0.0250\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 16.4537 - val_accuracy: 0.0250\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 16.4705 - val_accuracy: 0.0250\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 16.4871 - val_accuracy: 0.0250\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 16.5036 - val_accuracy: 0.0250\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 16.5200 - val_accuracy: 0.0250\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 135ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 16.5364 - val_accuracy: 0.0250\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 16.5528 - val_accuracy: 0.0250\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 9.9344e-04 - accuracy: 1.0000 - val_loss: 16.5692 - val_accuracy: 0.0250\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 9.6896e-04 - accuracy: 1.0000 - val_loss: 16.5857 - val_accuracy: 0.0250\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 9.4552e-04 - accuracy: 1.0000 - val_loss: 16.6022 - val_accuracy: 0.0250\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 9.2305e-04 - accuracy: 1.0000 - val_loss: 16.6188 - val_accuracy: 0.0250\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 9.0145e-04 - accuracy: 1.0000 - val_loss: 16.6355 - val_accuracy: 0.0250\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 8.8067e-04 - accuracy: 1.0000 - val_loss: 16.6523 - val_accuracy: 0.0250\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 8.6064e-04 - accuracy: 1.0000 - val_loss: 16.6692 - val_accuracy: 0.0250\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 8.4130e-04 - accuracy: 1.0000 - val_loss: 16.6861 - val_accuracy: 0.0250\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 8.2262e-04 - accuracy: 1.0000 - val_loss: 16.7033 - val_accuracy: 0.0250\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 8.0456e-04 - accuracy: 1.0000 - val_loss: 16.7205 - val_accuracy: 0.0250\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 7.8704e-04 - accuracy: 1.0000 - val_loss: 16.7378 - val_accuracy: 0.0250\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 7.7002e-04 - accuracy: 1.0000 - val_loss: 16.7552 - val_accuracy: 0.0250\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 7.5339e-04 - accuracy: 1.0000 - val_loss: 16.7727 - val_accuracy: 0.0250\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 7.3703e-04 - accuracy: 1.0000 - val_loss: 16.7902 - val_accuracy: 0.0250\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 7.2071e-04 - accuracy: 1.0000 - val_loss: 16.8077 - val_accuracy: 0.0250\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 7.0399e-04 - accuracy: 1.0000 - val_loss: 16.8247 - val_accuracy: 0.0250\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 6.8629e-04 - accuracy: 1.0000 - val_loss: 16.8412 - val_accuracy: 0.0250\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 6.6743e-04 - accuracy: 1.0000 - val_loss: 16.8568 - val_accuracy: 0.0250\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 6.4946e-04 - accuracy: 1.0000 - val_loss: 16.8728 - val_accuracy: 0.0250\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 6.3422e-04 - accuracy: 1.0000 - val_loss: 16.8894 - val_accuracy: 0.0250\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 6.2098e-04 - accuracy: 1.0000 - val_loss: 16.9064 - val_accuracy: 0.0250\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 110ms/step - loss: 6.0881e-04 - accuracy: 1.0000 - val_loss: 16.9239 - val_accuracy: 0.0250\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.9730e-04 - accuracy: 1.0000 - val_loss: 16.9412 - val_accuracy: 0.0250\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.8630e-04 - accuracy: 1.0000 - val_loss: 16.9586 - val_accuracy: 0.0250\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 5.7571e-04 - accuracy: 1.0000 - val_loss: 16.9760 - val_accuracy: 0.0250\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 5.6548e-04 - accuracy: 1.0000 - val_loss: 16.9931 - val_accuracy: 0.0250\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 5.5555e-04 - accuracy: 1.0000 - val_loss: 17.0102 - val_accuracy: 0.0250\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 5.4587e-04 - accuracy: 1.0000 - val_loss: 17.0271 - val_accuracy: 0.0250\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 5.3633e-04 - accuracy: 1.0000 - val_loss: 17.0438 - val_accuracy: 0.0250\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 5.2597e-04 - accuracy: 1.0000 - val_loss: 17.0487 - val_accuracy: 0.0250\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 5.1089e-04 - accuracy: 1.0000 - val_loss: 17.0970 - val_accuracy: 0.0250\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 5.1096e-04 - accuracy: 1.0000 - val_loss: 17.0844 - val_accuracy: 0.0250\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 4.9049e-04 - accuracy: 1.0000 - val_loss: 17.1216 - val_accuracy: 0.0250\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 4.7861e-04 - accuracy: 1.0000 - val_loss: 17.1203 - val_accuracy: 0.0250\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 4.6759e-04 - accuracy: 1.0000 - val_loss: 17.1437 - val_accuracy: 0.0250\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 4.5846e-04 - accuracy: 1.0000 - val_loss: 17.1447 - val_accuracy: 0.0250\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 4.4997e-04 - accuracy: 1.0000 - val_loss: 17.1671 - val_accuracy: 0.0250\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 4.4188e-04 - accuracy: 1.0000 - val_loss: 17.1680 - val_accuracy: 0.0250\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 4.3416e-04 - accuracy: 1.0000 - val_loss: 17.1961 - val_accuracy: 0.0250\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 112ms/step - loss: 4.2673e-04 - accuracy: 1.0000 - val_loss: 17.1911 - val_accuracy: 0.0250\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 4.1974e-04 - accuracy: 1.0000 - val_loss: 17.2318 - val_accuracy: 0.0250\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 4.1292e-04 - accuracy: 1.0000 - val_loss: 17.2177 - val_accuracy: 0.0250\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 4.0692e-04 - accuracy: 1.0000 - val_loss: 17.2706 - val_accuracy: 0.0250\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 4.0006e-04 - accuracy: 1.0000 - val_loss: 17.2433 - val_accuracy: 0.0250\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 3.9388e-04 - accuracy: 1.0000 - val_loss: 17.2879 - val_accuracy: 0.0250\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 3.8604e-04 - accuracy: 1.0000 - val_loss: 17.2625 - val_accuracy: 0.0250\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 3.7922e-04 - accuracy: 1.0000 - val_loss: 17.2955 - val_accuracy: 0.0250\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 3.7200e-04 - accuracy: 1.0000 - val_loss: 17.2851 - val_accuracy: 0.0250\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 3.6551e-04 - accuracy: 1.0000 - val_loss: 17.3132 - val_accuracy: 0.0250\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 3.5907e-04 - accuracy: 1.0000 - val_loss: 17.3089 - val_accuracy: 0.0250\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 3.5276e-04 - accuracy: 1.0000 - val_loss: 17.3346 - val_accuracy: 0.0250\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 3.4649e-04 - accuracy: 1.0000 - val_loss: 17.3308 - val_accuracy: 0.0250\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 3.4024e-04 - accuracy: 1.0000 - val_loss: 17.3574 - val_accuracy: 0.0250\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 3.3399e-04 - accuracy: 1.0000 - val_loss: 17.3507 - val_accuracy: 0.0250\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 3.2772e-04 - accuracy: 1.0000 - val_loss: 17.3820 - val_accuracy: 0.0250\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 3.2141e-04 - accuracy: 1.0000 - val_loss: 17.3669 - val_accuracy: 0.0250\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 3.1510e-04 - accuracy: 1.0000 - val_loss: 17.4106 - val_accuracy: 0.0250\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 3.0879e-04 - accuracy: 1.0000 - val_loss: 17.3774 - val_accuracy: 0.0250\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 3.0264e-04 - accuracy: 1.0000 - val_loss: 17.4510 - val_accuracy: 0.0250\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 2.9667e-04 - accuracy: 1.0000 - val_loss: 17.3840 - val_accuracy: 0.0250\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 2.9121e-04 - accuracy: 1.0000 - val_loss: 17.5073 - val_accuracy: 0.0250\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 2.8613e-04 - accuracy: 1.0000 - val_loss: 17.3932 - val_accuracy: 0.0250\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 2.8096e-04 - accuracy: 1.0000 - val_loss: 17.5469 - val_accuracy: 0.0250\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 2.7710e-04 - accuracy: 1.0000 - val_loss: 17.4067 - val_accuracy: 0.0250\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 2.7132e-04 - accuracy: 1.0000 - val_loss: 17.5573 - val_accuracy: 0.0250\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 2.6683e-04 - accuracy: 1.0000 - val_loss: 17.4073 - val_accuracy: 0.0250\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 2.6139e-04 - accuracy: 1.0000 - val_loss: 17.5351 - val_accuracy: 0.0250\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 2.5746e-04 - accuracy: 1.0000 - val_loss: 17.4210 - val_accuracy: 0.0250\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 2.5427e-04 - accuracy: 1.0000 - val_loss: 17.5362 - val_accuracy: 0.0250\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 2.5236e-04 - accuracy: 1.0000 - val_loss: 17.4312 - val_accuracy: 0.0250\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 2.5228e-04 - accuracy: 1.0000 - val_loss: 17.5609 - val_accuracy: 0.0250\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 2.5705e-04 - accuracy: 1.0000 - val_loss: 17.4204 - val_accuracy: 0.0250\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 2.7102e-04 - accuracy: 1.0000 - val_loss: 17.6161 - val_accuracy: 0.0250\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 3.2527e-04 - accuracy: 1.0000 - val_loss: 17.3470 - val_accuracy: 0.0250\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 6.2985e-04 - accuracy: 1.0000 - val_loss: 17.7106 - val_accuracy: 0.0250\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0216 - accuracy: 0.9975 - val_loss: 16.1207 - val_accuracy: 0.0200\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 0.2011 - accuracy: 0.9287 - val_loss: 16.4191 - val_accuracy: 0.0250\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 16.0780 - val_accuracy: 0.0250\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 16.0452 - val_accuracy: 0.0250\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 16.0222 - val_accuracy: 0.0250\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 16.0053 - val_accuracy: 0.0250\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 15.9928 - val_accuracy: 0.0250\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 15.9840 - val_accuracy: 0.0250\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 15.9780 - val_accuracy: 0.0250\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 15.9744 - val_accuracy: 0.0250\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 15.9728 - val_accuracy: 0.0250\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 15.9729 - val_accuracy: 0.0250\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 9.6338e-04 - accuracy: 1.0000 - val_loss: 15.9747 - val_accuracy: 0.0250\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 9.0293e-04 - accuracy: 1.0000 - val_loss: 15.9782 - val_accuracy: 0.0250\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 8.4735e-04 - accuracy: 1.0000 - val_loss: 15.9826 - val_accuracy: 0.0250\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 8.0033e-04 - accuracy: 1.0000 - val_loss: 15.9870 - val_accuracy: 0.0250\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 133ms/step - loss: 7.5973e-04 - accuracy: 1.0000 - val_loss: 15.9921 - val_accuracy: 0.0250\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 7.2332e-04 - accuracy: 1.0000 - val_loss: 15.9980 - val_accuracy: 0.0250\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 6.9036e-04 - accuracy: 1.0000 - val_loss: 16.0048 - val_accuracy: 0.0250\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 6.6034e-04 - accuracy: 1.0000 - val_loss: 16.0124 - val_accuracy: 0.0250\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 6.3286e-04 - accuracy: 1.0000 - val_loss: 16.0207 - val_accuracy: 0.0250\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 6.0758e-04 - accuracy: 1.0000 - val_loss: 16.0297 - val_accuracy: 0.0250\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 5.8422e-04 - accuracy: 1.0000 - val_loss: 16.0394 - val_accuracy: 0.0250\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 5.6249e-04 - accuracy: 1.0000 - val_loss: 16.0499 - val_accuracy: 0.0250\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 5.4216e-04 - accuracy: 1.0000 - val_loss: 16.0611 - val_accuracy: 0.0250\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 5.2298e-04 - accuracy: 1.0000 - val_loss: 16.0731 - val_accuracy: 0.0250\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 5.0472e-04 - accuracy: 1.0000 - val_loss: 16.0859 - val_accuracy: 0.0250\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 4.8721e-04 - accuracy: 1.0000 - val_loss: 16.0993 - val_accuracy: 0.0250\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 4.7028e-04 - accuracy: 1.0000 - val_loss: 16.1136 - val_accuracy: 0.0250\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 173ms/step - loss: 4.5392e-04 - accuracy: 1.0000 - val_loss: 16.1285 - val_accuracy: 0.0250\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 4.3833e-04 - accuracy: 1.0000 - val_loss: 16.1440 - val_accuracy: 0.0250\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 4.2389e-04 - accuracy: 1.0000 - val_loss: 16.1600 - val_accuracy: 0.0250\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 4.1078e-04 - accuracy: 1.0000 - val_loss: 16.1764 - val_accuracy: 0.0250\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 3.9873e-04 - accuracy: 1.0000 - val_loss: 16.1933 - val_accuracy: 0.0250\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 3.8736e-04 - accuracy: 1.0000 - val_loss: 16.2106 - val_accuracy: 0.0250\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 3.7625e-04 - accuracy: 1.0000 - val_loss: 16.2283 - val_accuracy: 0.0250\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 3.6557e-04 - accuracy: 1.0000 - val_loss: 16.2465 - val_accuracy: 0.0250\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 3.5643e-04 - accuracy: 1.0000 - val_loss: 16.2653 - val_accuracy: 0.0250\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 3.4848e-04 - accuracy: 1.0000 - val_loss: 16.2844 - val_accuracy: 0.0250\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 3.4102e-04 - accuracy: 1.0000 - val_loss: 16.3037 - val_accuracy: 0.0250\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 3.3388e-04 - accuracy: 1.0000 - val_loss: 16.3233 - val_accuracy: 0.0250\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 3.2702e-04 - accuracy: 1.0000 - val_loss: 16.3431 - val_accuracy: 0.0250\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 3.2040e-04 - accuracy: 1.0000 - val_loss: 16.3633 - val_accuracy: 0.0250\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 3.1398e-04 - accuracy: 1.0000 - val_loss: 16.3837 - val_accuracy: 0.0250\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 3.0778e-04 - accuracy: 1.0000 - val_loss: 16.4043 - val_accuracy: 0.0250\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 3.0177e-04 - accuracy: 1.0000 - val_loss: 16.4253 - val_accuracy: 0.0250\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 131ms/step - loss: 2.9593e-04 - accuracy: 1.0000 - val_loss: 16.4465 - val_accuracy: 0.0250\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 2.9026e-04 - accuracy: 1.0000 - val_loss: 16.4680 - val_accuracy: 0.0250\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 2.8474e-04 - accuracy: 1.0000 - val_loss: 16.4897 - val_accuracy: 0.0250\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 2.7937e-04 - accuracy: 1.0000 - val_loss: 16.5118 - val_accuracy: 0.0250\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 129ms/step - loss: 2.7414e-04 - accuracy: 1.0000 - val_loss: 16.5340 - val_accuracy: 0.0250\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 2.6904e-04 - accuracy: 1.0000 - val_loss: 16.5565 - val_accuracy: 0.0250\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 2.6407e-04 - accuracy: 1.0000 - val_loss: 16.5790 - val_accuracy: 0.0250\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 2.5922e-04 - accuracy: 1.0000 - val_loss: 16.6018 - val_accuracy: 0.0250\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 2.5449e-04 - accuracy: 1.0000 - val_loss: 16.6248 - val_accuracy: 0.0250\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 2.4987e-04 - accuracy: 1.0000 - val_loss: 16.6478 - val_accuracy: 0.0250\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 2.4535e-04 - accuracy: 1.0000 - val_loss: 16.6709 - val_accuracy: 0.0250\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 2.4095e-04 - accuracy: 1.0000 - val_loss: 16.6940 - val_accuracy: 0.0250\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 2.3665e-04 - accuracy: 1.0000 - val_loss: 16.7170 - val_accuracy: 0.0250\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 2.3244e-04 - accuracy: 1.0000 - val_loss: 16.7402 - val_accuracy: 0.0250\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 2.2834e-04 - accuracy: 1.0000 - val_loss: 16.7631 - val_accuracy: 0.0250\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 2.2433e-04 - accuracy: 1.0000 - val_loss: 16.7860 - val_accuracy: 0.0250\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 2.2041e-04 - accuracy: 1.0000 - val_loss: 16.8089 - val_accuracy: 0.0250\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 124ms/step - loss: 2.1658e-04 - accuracy: 1.0000 - val_loss: 16.8316 - val_accuracy: 0.0250\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 2.1284e-04 - accuracy: 1.0000 - val_loss: 16.8540 - val_accuracy: 0.0250\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 2.0918e-04 - accuracy: 1.0000 - val_loss: 16.8764 - val_accuracy: 0.0250\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 127ms/step - loss: 2.0561e-04 - accuracy: 1.0000 - val_loss: 16.8985 - val_accuracy: 0.0250\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 2.0212e-04 - accuracy: 1.0000 - val_loss: 16.9205 - val_accuracy: 0.0250\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 1.9870e-04 - accuracy: 1.0000 - val_loss: 16.9420 - val_accuracy: 0.0250\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 116ms/step - loss: 1.9537e-04 - accuracy: 1.0000 - val_loss: 16.9634 - val_accuracy: 0.0250\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 1.9210e-04 - accuracy: 1.0000 - val_loss: 16.9844 - val_accuracy: 0.0250\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 1.8890e-04 - accuracy: 1.0000 - val_loss: 17.0051 - val_accuracy: 0.0250\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 1.8578e-04 - accuracy: 1.0000 - val_loss: 17.0254 - val_accuracy: 0.0250\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 119ms/step - loss: 1.8273e-04 - accuracy: 1.0000 - val_loss: 17.0454 - val_accuracy: 0.0250\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 1.7973e-04 - accuracy: 1.0000 - val_loss: 17.0648 - val_accuracy: 0.0250\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 1.7679e-04 - accuracy: 1.0000 - val_loss: 17.0842 - val_accuracy: 0.0250\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 130ms/step - loss: 1.7391e-04 - accuracy: 1.0000 - val_loss: 17.1030 - val_accuracy: 0.0250\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 1.7107e-04 - accuracy: 1.0000 - val_loss: 17.1214 - val_accuracy: 0.0250\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.6831e-04 - accuracy: 1.0000 - val_loss: 17.1391 - val_accuracy: 0.0250\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.6559e-04 - accuracy: 1.0000 - val_loss: 17.1568 - val_accuracy: 0.0250\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 121ms/step - loss: 1.6294e-04 - accuracy: 1.0000 - val_loss: 17.1738 - val_accuracy: 0.0250\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 128ms/step - loss: 1.6035e-04 - accuracy: 1.0000 - val_loss: 17.1905 - val_accuracy: 0.0250\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 1.5783e-04 - accuracy: 1.0000 - val_loss: 17.2065 - val_accuracy: 0.0250\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.5538e-04 - accuracy: 1.0000 - val_loss: 17.2224 - val_accuracy: 0.0250\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 1.5298e-04 - accuracy: 1.0000 - val_loss: 17.2373 - val_accuracy: 0.0250\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.5063e-04 - accuracy: 1.0000 - val_loss: 17.2519 - val_accuracy: 0.0250\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.4835e-04 - accuracy: 1.0000 - val_loss: 17.2660 - val_accuracy: 0.0250\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 126ms/step - loss: 1.4610e-04 - accuracy: 1.0000 - val_loss: 17.2792 - val_accuracy: 0.0250\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.4390e-04 - accuracy: 1.0000 - val_loss: 17.2921 - val_accuracy: 0.0250\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 132ms/step - loss: 1.4173e-04 - accuracy: 1.0000 - val_loss: 17.3041 - val_accuracy: 0.0250\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 111ms/step - loss: 1.3962e-04 - accuracy: 1.0000 - val_loss: 17.3157 - val_accuracy: 0.0250\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 125ms/step - loss: 1.3753e-04 - accuracy: 1.0000 - val_loss: 17.3270 - val_accuracy: 0.0250\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 117ms/step - loss: 1.3548e-04 - accuracy: 1.0000 - val_loss: 17.3376 - val_accuracy: 0.0250\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.3345e-04 - accuracy: 1.0000 - val_loss: 17.3477 - val_accuracy: 0.0250\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.3146e-04 - accuracy: 1.0000 - val_loss: 17.3575 - val_accuracy: 0.0250\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 122ms/step - loss: 1.2950e-04 - accuracy: 1.0000 - val_loss: 17.3673 - val_accuracy: 0.0250\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 123ms/step - loss: 1.2757e-04 - accuracy: 1.0000 - val_loss: 17.3769 - val_accuracy: 0.0250\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 113ms/step - loss: 1.2566e-04 - accuracy: 1.0000 - val_loss: 17.3868 - val_accuracy: 0.0250\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 134ms/step - loss: 1.2378e-04 - accuracy: 1.0000 - val_loss: 17.3964 - val_accuracy: 0.0250\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 1.2193e-04 - accuracy: 1.0000 - val_loss: 17.4068 - val_accuracy: 0.0250\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.2009e-04 - accuracy: 1.0000 - val_loss: 17.4167 - val_accuracy: 0.0250\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 1.1829e-04 - accuracy: 1.0000 - val_loss: 17.4275 - val_accuracy: 0.0250\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 1.1650e-04 - accuracy: 1.0000 - val_loss: 17.4376 - val_accuracy: 0.0250\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 120ms/step - loss: 1.1474e-04 - accuracy: 1.0000 - val_loss: 17.4487 - val_accuracy: 0.0250\n",
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_9_layer_call_fn, lstm_cell_9_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: s2s/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: s2s/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fb9d36230d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fb9a50ff7d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZc0QC9pCfUS"
      },
      "source": [
        "## Run inference (sampling)\n",
        "\n",
        "1. encode input and retrieve initial decoder state\n",
        "2. run one step of decoder with this initial state\n",
        "and a \"start of sequence\" token as target.\n",
        "Output will be the next target token.\n",
        "3. Repeat with the current target token and current states\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1TriXq7nnD_q",
        "outputId": "9ce61257-4c15-4125-fe17-ba687b751cf1"
      },
      "source": [
        "plt.savefig(\"plots.pdf\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeoEYHvnCfUS"
      },
      "source": [
        "# Define sampling models\n",
        "# Restore the model and construct the encoder and decoder.\n",
        "model = keras.models.load_model(\"s2s\")\n",
        "\n",
        "encoder_inputs = model.input[0]  # input_1\n",
        "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_inputs = model.input[1]  # input_2\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model.layers[3]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model.layers[4]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "\n"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imhvUjlLH7uN",
        "outputId": "69bd38c8-95c8-4854-c171-6f47b746fdb5"
      },
      "source": [
        "reverse_input_char_index"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0,\n",
              " 1: 1,\n",
              " 2: 2,\n",
              " 3: 3,\n",
              " 4: 4,\n",
              " 5: 5,\n",
              " 6: 6,\n",
              " 7: 7,\n",
              " 8: 8,\n",
              " 9: 9,\n",
              " 10: 10,\n",
              " 11: 11,\n",
              " 12: 12,\n",
              " 13: 13,\n",
              " 14: 14,\n",
              " 15: 15,\n",
              " 16: 16,\n",
              " 17: 17,\n",
              " 18: 18,\n",
              " 19: 19,\n",
              " 20: 20,\n",
              " 21: 21,\n",
              " 22: 22,\n",
              " 23: 23,\n",
              " 24: 24,\n",
              " 25: 25,\n",
              " 26: 26,\n",
              " 27: 27,\n",
              " 28: 28,\n",
              " 29: 29,\n",
              " 30: 30,\n",
              " 31: 31,\n",
              " 32: 32,\n",
              " 33: 33,\n",
              " 34: 34,\n",
              " 35: 35,\n",
              " 36: 36,\n",
              " 37: 37,\n",
              " 38: 38,\n",
              " 39: 39,\n",
              " 40: 40,\n",
              " 41: 41,\n",
              " 42: 42,\n",
              " 43: 43,\n",
              " 44: 44,\n",
              " 45: 45,\n",
              " 46: 46,\n",
              " 47: 47,\n",
              " 48: 48,\n",
              " 49: 49,\n",
              " 50: 50,\n",
              " 51: 51,\n",
              " 52: 52,\n",
              " 53: 53,\n",
              " 54: 54,\n",
              " 55: 55,\n",
              " 56: 56,\n",
              " 57: 57,\n",
              " 58: 58,\n",
              " 59: 59,\n",
              " 60: 60,\n",
              " 61: 61,\n",
              " 62: 62,\n",
              " 63: 63,\n",
              " 64: 64,\n",
              " 65: 65,\n",
              " 66: 66,\n",
              " 67: 67,\n",
              " 68: 68,\n",
              " 69: 69,\n",
              " 70: 70,\n",
              " 71: 71,\n",
              " 72: 72,\n",
              " 73: 73,\n",
              " 74: 74,\n",
              " 75: 75,\n",
              " 76: 76,\n",
              " 77: 77,\n",
              " 78: 78,\n",
              " 79: 79,\n",
              " 80: 80,\n",
              " 81: 81,\n",
              " 82: 82,\n",
              " 83: 83,\n",
              " 84: 84,\n",
              " 85: 85,\n",
              " 86: 86,\n",
              " 87: 87,\n",
              " 88: 88,\n",
              " 89: 89,\n",
              " 90: 90,\n",
              " 91: 91,\n",
              " 92: 92,\n",
              " 93: 93,\n",
              " 94: 94,\n",
              " 95: 95,\n",
              " 96: 96,\n",
              " 97: 97,\n",
              " 98: 98,\n",
              " 99: 99,\n",
              " 100: 100,\n",
              " 101: 101,\n",
              " 102: 102,\n",
              " 103: 103,\n",
              " 104: 104,\n",
              " 105: 105,\n",
              " 106: 106,\n",
              " 107: 107,\n",
              " 108: 108,\n",
              " 109: 109,\n",
              " 110: 110,\n",
              " 111: 111,\n",
              " 112: 112,\n",
              " 113: 113,\n",
              " 114: 114,\n",
              " 115: 115,\n",
              " 116: 116,\n",
              " 117: 117,\n",
              " 118: 118,\n",
              " 119: 119,\n",
              " 120: 120,\n",
              " 121: 121,\n",
              " 122: 122,\n",
              " 123: 123,\n",
              " 124: 124,\n",
              " 125: 125,\n",
              " 126: 126,\n",
              " 127: 127,\n",
              " 128: 128,\n",
              " 129: 129,\n",
              " 130: 130,\n",
              " 131: 131,\n",
              " 132: 132,\n",
              " 133: 133,\n",
              " 134: 134,\n",
              " 135: 135,\n",
              " 136: 136,\n",
              " 137: 137,\n",
              " 138: 138,\n",
              " 139: 139,\n",
              " 140: 140,\n",
              " 141: 141,\n",
              " 142: 142,\n",
              " 143: 143,\n",
              " 144: 144,\n",
              " 145: 145,\n",
              " 146: 146,\n",
              " 147: 147,\n",
              " 148: 148,\n",
              " 149: 149,\n",
              " 150: 150,\n",
              " 151: 151,\n",
              " 152: 152,\n",
              " 153: 153,\n",
              " 154: 154,\n",
              " 155: 155,\n",
              " 156: 156,\n",
              " 157: 157,\n",
              " 158: 158,\n",
              " 159: 159,\n",
              " 160: 160,\n",
              " 161: 161,\n",
              " 162: 162,\n",
              " 163: 163,\n",
              " 164: 164,\n",
              " 165: 165,\n",
              " 166: 166,\n",
              " 167: 167,\n",
              " 168: 168,\n",
              " 169: 169,\n",
              " 170: 170,\n",
              " 171: 171,\n",
              " 172: 172,\n",
              " 173: 173,\n",
              " 174: 174,\n",
              " 175: 175,\n",
              " 176: 176,\n",
              " 177: 177,\n",
              " 178: 178,\n",
              " 179: 179,\n",
              " 180: 180,\n",
              " 181: 181,\n",
              " 182: 182,\n",
              " 183: 183,\n",
              " 184: 184,\n",
              " 185: 185,\n",
              " 186: 186,\n",
              " 187: 187,\n",
              " 188: 188,\n",
              " 189: 189,\n",
              " 190: 190,\n",
              " 191: 191,\n",
              " 192: 192,\n",
              " 193: 193,\n",
              " 194: 194,\n",
              " 195: 195,\n",
              " 196: 196,\n",
              " 197: 197,\n",
              " 198: 198,\n",
              " 199: 199,\n",
              " 200: 200,\n",
              " 201: 201,\n",
              " 202: 202,\n",
              " 203: 203,\n",
              " 204: 204,\n",
              " 205: 205,\n",
              " 206: 206,\n",
              " 207: 207,\n",
              " 208: 208,\n",
              " 209: 209,\n",
              " 210: 210,\n",
              " 211: 211,\n",
              " 212: 212,\n",
              " 213: 213,\n",
              " 214: 214,\n",
              " 215: 215,\n",
              " 216: 216,\n",
              " 217: 217,\n",
              " 218: 218,\n",
              " 219: 219,\n",
              " 220: 220,\n",
              " 221: 221,\n",
              " 222: 222,\n",
              " 223: 223,\n",
              " 224: 224,\n",
              " 225: 225,\n",
              " 226: 226,\n",
              " 227: 227,\n",
              " 228: 228,\n",
              " 229: 229,\n",
              " 230: 230,\n",
              " 231: 231,\n",
              " 232: 232,\n",
              " 233: 233,\n",
              " 234: 234,\n",
              " 235: 235,\n",
              " 236: 236,\n",
              " 237: 237,\n",
              " 238: 238,\n",
              " 239: 239,\n",
              " 240: 240,\n",
              " 241: 241,\n",
              " 242: 242,\n",
              " 243: 243,\n",
              " 244: 244,\n",
              " 245: 245,\n",
              " 246: 246,\n",
              " 247: 247,\n",
              " 248: 248,\n",
              " 249: 249,\n",
              " 250: 250,\n",
              " 251: 251,\n",
              " 252: 252,\n",
              " 253: 253,\n",
              " 254: 254,\n",
              " 255: 255,\n",
              " 256: 256,\n",
              " 257: 257,\n",
              " 258: 258,\n",
              " 259: 259,\n",
              " 260: 260,\n",
              " 261: 261,\n",
              " 262: 262,\n",
              " 263: 263,\n",
              " 264: 264,\n",
              " 265: 265,\n",
              " 266: 266,\n",
              " 267: 267,\n",
              " 268: 268,\n",
              " 269: 269,\n",
              " 270: 270,\n",
              " 271: 271,\n",
              " 272: 272,\n",
              " 273: 273,\n",
              " 274: 274,\n",
              " 275: 275,\n",
              " 276: 276,\n",
              " 277: 277,\n",
              " 278: 278,\n",
              " 279: 279,\n",
              " 280: 280,\n",
              " 281: 281,\n",
              " 282: 282,\n",
              " 283: 283,\n",
              " 284: 284,\n",
              " 285: 285,\n",
              " 286: 286,\n",
              " 287: 287,\n",
              " 288: 288,\n",
              " 289: 289,\n",
              " 290: 290,\n",
              " 291: 291,\n",
              " 292: 292,\n",
              " 293: 293,\n",
              " 294: 294,\n",
              " 295: 295,\n",
              " 296: 296,\n",
              " 297: 297,\n",
              " 298: 298,\n",
              " 299: 299,\n",
              " 300: 300,\n",
              " 301: 301,\n",
              " 302: 302,\n",
              " 303: 303,\n",
              " 304: 304,\n",
              " 305: 305,\n",
              " 306: 306,\n",
              " 307: 307,\n",
              " 308: 308,\n",
              " 309: 309,\n",
              " 310: 310,\n",
              " 311: 311,\n",
              " 312: 312,\n",
              " 313: 313,\n",
              " 314: 314,\n",
              " 315: 315,\n",
              " 316: 316,\n",
              " 317: 317,\n",
              " 318: 318,\n",
              " 319: 319,\n",
              " 320: 320,\n",
              " 321: 321,\n",
              " 322: 322,\n",
              " 323: 323,\n",
              " 324: 324,\n",
              " 325: 325,\n",
              " 326: 326,\n",
              " 327: 327,\n",
              " 328: 328,\n",
              " 329: 329,\n",
              " 330: 330,\n",
              " 331: 331,\n",
              " 332: 332,\n",
              " 333: 333,\n",
              " 334: 334,\n",
              " 335: 335,\n",
              " 336: 336,\n",
              " 337: 337,\n",
              " 338: 338,\n",
              " 339: 339,\n",
              " 340: 340,\n",
              " 341: 341,\n",
              " 342: 342,\n",
              " 343: 343,\n",
              " 344: 344,\n",
              " 345: 345,\n",
              " 346: 346,\n",
              " 347: 347,\n",
              " 348: 348,\n",
              " 349: 349,\n",
              " 350: 350,\n",
              " 351: 351,\n",
              " 352: 352,\n",
              " 353: 353,\n",
              " 354: 354,\n",
              " 355: 355,\n",
              " 356: 356,\n",
              " 357: 357,\n",
              " 358: 358,\n",
              " 359: 359,\n",
              " 360: 360,\n",
              " 361: 361,\n",
              " 362: 362,\n",
              " 363: 363,\n",
              " 364: 364,\n",
              " 365: 365,\n",
              " 366: 366,\n",
              " 367: 367,\n",
              " 368: 368,\n",
              " 369: 369,\n",
              " 370: 370,\n",
              " 371: 371,\n",
              " 372: 372,\n",
              " 373: 373,\n",
              " 374: 374,\n",
              " 375: 375,\n",
              " 376: 376,\n",
              " 377: 377,\n",
              " 378: 378,\n",
              " 379: 379,\n",
              " 380: 380,\n",
              " 381: 381,\n",
              " 382: 382,\n",
              " 383: 383,\n",
              " 384: 384,\n",
              " 385: 385,\n",
              " 386: 386,\n",
              " 387: 387,\n",
              " 388: 388,\n",
              " 389: 389,\n",
              " 390: 390,\n",
              " 391: 391,\n",
              " 392: 392,\n",
              " 393: 393,\n",
              " 394: 394,\n",
              " 395: 395,\n",
              " 396: 396,\n",
              " 397: 397,\n",
              " 398: 398,\n",
              " 399: 399,\n",
              " 400: 400,\n",
              " 401: 401,\n",
              " 402: 402,\n",
              " 403: 403,\n",
              " 404: 404,\n",
              " 405: 405,\n",
              " 406: 406,\n",
              " 407: 407,\n",
              " 408: 408,\n",
              " 409: 409,\n",
              " 410: 410,\n",
              " 411: 411,\n",
              " 412: 412,\n",
              " 413: 413,\n",
              " 414: 414,\n",
              " 415: 415,\n",
              " 416: 416,\n",
              " 417: 417,\n",
              " 418: 418,\n",
              " 419: 419,\n",
              " 420: 420,\n",
              " 421: 421,\n",
              " 422: 422,\n",
              " 423: 423,\n",
              " 424: 424,\n",
              " 425: 425,\n",
              " 426: 426,\n",
              " 427: 427,\n",
              " 428: 428,\n",
              " 429: 429,\n",
              " 430: 430,\n",
              " 431: 431,\n",
              " 432: 432,\n",
              " 433: 433,\n",
              " 434: 434,\n",
              " 435: 435,\n",
              " 436: 436,\n",
              " 437: 437,\n",
              " 438: 438,\n",
              " 439: 439,\n",
              " 440: 440,\n",
              " 441: 441,\n",
              " 442: 442,\n",
              " 443: 443,\n",
              " 444: 444,\n",
              " 445: 445,\n",
              " 446: 446,\n",
              " 447: 447,\n",
              " 448: 448,\n",
              " 449: 449,\n",
              " 450: 450,\n",
              " 451: 451,\n",
              " 452: 452,\n",
              " 453: 453,\n",
              " 454: 454,\n",
              " 455: 455,\n",
              " 456: 456,\n",
              " 457: 457,\n",
              " 458: 458,\n",
              " 459: 459,\n",
              " 460: 460,\n",
              " 461: 461,\n",
              " 462: 462,\n",
              " 463: 463,\n",
              " 464: 464,\n",
              " 465: 465,\n",
              " 466: 466,\n",
              " 467: 467,\n",
              " 468: 468,\n",
              " 469: 469,\n",
              " 470: 470,\n",
              " 471: 471,\n",
              " 472: 472,\n",
              " 473: 473,\n",
              " 474: 474,\n",
              " 475: 475,\n",
              " 476: 476,\n",
              " 477: 477,\n",
              " 478: 478,\n",
              " 479: 479,\n",
              " 480: 480,\n",
              " 481: 481,\n",
              " 482: 482,\n",
              " 483: 483,\n",
              " 484: 484,\n",
              " 485: 485,\n",
              " 486: 486,\n",
              " 487: 487,\n",
              " 488: 488,\n",
              " 489: 489,\n",
              " 490: 490,\n",
              " 491: 491,\n",
              " 492: 492,\n",
              " 493: 493,\n",
              " 494: 494,\n",
              " 495: 495,\n",
              " 496: 496,\n",
              " 497: 497,\n",
              " 498: 498,\n",
              " 499: 499,\n",
              " 500: 500,\n",
              " 501: 501,\n",
              " 502: 502,\n",
              " 503: 503,\n",
              " 504: 504,\n",
              " 505: 505,\n",
              " 506: 506,\n",
              " 507: 507,\n",
              " 508: 508,\n",
              " 509: 509,\n",
              " 510: 510,\n",
              " 511: 511,\n",
              " 512: 512,\n",
              " 513: 513,\n",
              " 514: 514,\n",
              " 515: 515,\n",
              " 516: 516,\n",
              " 517: 517,\n",
              " 518: 518,\n",
              " 519: 519,\n",
              " 520: 520,\n",
              " 521: 521,\n",
              " 522: 522,\n",
              " 523: 523,\n",
              " 524: 524,\n",
              " 525: 525,\n",
              " 526: 526,\n",
              " 527: 527,\n",
              " 528: 528,\n",
              " 529: 529,\n",
              " 530: 530,\n",
              " 531: 531,\n",
              " 532: 532,\n",
              " 533: 533,\n",
              " 534: 534,\n",
              " 535: 535,\n",
              " 536: 536,\n",
              " 537: 537,\n",
              " 538: 538,\n",
              " 539: 539,\n",
              " 540: 540,\n",
              " 541: 541,\n",
              " 542: 542,\n",
              " 543: 543,\n",
              " 544: 544,\n",
              " 545: 545,\n",
              " 546: 546,\n",
              " 547: 547,\n",
              " 548: 548,\n",
              " 549: 549,\n",
              " 550: 550,\n",
              " 551: 551,\n",
              " 552: 552,\n",
              " 553: 553,\n",
              " 554: 554,\n",
              " 555: 555,\n",
              " 556: 556,\n",
              " 557: 557,\n",
              " 558: 558,\n",
              " 559: 559,\n",
              " 560: 560,\n",
              " 561: 561,\n",
              " 562: 562,\n",
              " 563: 563,\n",
              " 564: 564,\n",
              " 565: 565,\n",
              " 566: 566,\n",
              " 567: 567,\n",
              " 568: 568,\n",
              " 569: 569,\n",
              " 570: 570,\n",
              " 571: 571,\n",
              " 572: 572,\n",
              " 573: 573,\n",
              " 574: 574,\n",
              " 575: 575,\n",
              " 576: 576,\n",
              " 577: 577,\n",
              " 578: 578,\n",
              " 579: 579,\n",
              " 580: 580,\n",
              " 581: 581,\n",
              " 582: 582,\n",
              " 583: 583,\n",
              " 584: 584,\n",
              " 585: 585,\n",
              " 586: 586,\n",
              " 587: 587,\n",
              " 588: 588,\n",
              " 589: 589,\n",
              " 590: 590,\n",
              " 591: 591,\n",
              " 592: 592,\n",
              " 593: 593,\n",
              " 594: 594,\n",
              " 595: 595,\n",
              " 596: 596,\n",
              " 597: 597,\n",
              " 598: 598,\n",
              " 599: 599,\n",
              " 600: 600,\n",
              " 601: 601,\n",
              " 602: 602,\n",
              " 603: 603,\n",
              " 604: 604,\n",
              " 605: 605,\n",
              " 606: 606,\n",
              " 607: 607,\n",
              " 608: 608,\n",
              " 609: 609,\n",
              " 610: 610,\n",
              " 611: 611,\n",
              " 612: 612,\n",
              " 613: 613,\n",
              " 614: 614,\n",
              " 615: 615,\n",
              " 616: 616,\n",
              " 617: 617,\n",
              " 618: 618,\n",
              " 619: 619,\n",
              " 620: 620,\n",
              " 621: 621,\n",
              " 622: 622,\n",
              " 623: 623,\n",
              " 624: 624,\n",
              " 625: 625,\n",
              " 626: 626,\n",
              " 627: 627,\n",
              " 628: 628,\n",
              " 629: 629,\n",
              " 630: 630,\n",
              " 631: 631,\n",
              " 632: 632,\n",
              " 633: 633,\n",
              " 634: 634,\n",
              " 635: 635,\n",
              " 636: 636,\n",
              " 637: 637,\n",
              " 638: 638,\n",
              " 639: 639,\n",
              " 640: 640,\n",
              " 641: 641,\n",
              " 642: 642,\n",
              " 643: 643,\n",
              " 644: 644,\n",
              " 645: 645,\n",
              " 646: 646,\n",
              " 647: 647,\n",
              " 648: 648,\n",
              " 649: 649,\n",
              " 650: 650,\n",
              " 651: 651,\n",
              " 652: 652,\n",
              " 653: 653,\n",
              " 654: 654,\n",
              " 655: 655,\n",
              " 656: 656,\n",
              " 657: 657,\n",
              " 658: 658,\n",
              " 659: 659,\n",
              " 660: 660,\n",
              " 661: 661,\n",
              " 662: 662,\n",
              " 663: 663,\n",
              " 664: 664,\n",
              " 665: 665,\n",
              " 666: 666,\n",
              " 667: 667,\n",
              " 668: 668,\n",
              " 669: 669,\n",
              " 670: 670,\n",
              " 671: 671,\n",
              " 672: 672,\n",
              " 673: 673,\n",
              " 674: 674,\n",
              " 675: 675,\n",
              " 676: 676,\n",
              " 677: 677,\n",
              " 678: 678,\n",
              " 679: 679,\n",
              " 680: 680,\n",
              " 681: 681,\n",
              " 682: 682,\n",
              " 683: 683,\n",
              " 684: 684,\n",
              " 685: 685,\n",
              " 686: 686,\n",
              " 687: 687,\n",
              " 688: 688,\n",
              " 689: 689,\n",
              " 690: 690,\n",
              " 691: 691,\n",
              " 692: 692,\n",
              " 693: 693,\n",
              " 694: 694,\n",
              " 695: 695,\n",
              " 696: 696,\n",
              " 697: 697,\n",
              " 698: 698,\n",
              " 699: 699,\n",
              " 700: 700,\n",
              " 701: 701,\n",
              " 702: 702,\n",
              " 703: 703,\n",
              " 704: 704,\n",
              " 705: 705,\n",
              " 706: 706,\n",
              " 707: 707,\n",
              " 708: 708,\n",
              " 709: 709,\n",
              " 710: 710,\n",
              " 711: 711,\n",
              " 712: 712,\n",
              " 713: 713,\n",
              " 714: 714,\n",
              " 715: 715,\n",
              " 716: 716,\n",
              " 717: 717,\n",
              " 718: 718,\n",
              " 719: 719,\n",
              " 720: 720,\n",
              " 721: 721,\n",
              " 722: 722,\n",
              " 723: 723,\n",
              " 724: 724,\n",
              " 725: 725,\n",
              " 726: 726,\n",
              " 727: 727,\n",
              " 728: 728,\n",
              " 729: 729,\n",
              " 730: 730,\n",
              " 731: 731,\n",
              " 732: 732,\n",
              " 733: 733,\n",
              " 734: 734,\n",
              " 735: 735,\n",
              " 736: 736,\n",
              " 737: 737,\n",
              " 738: 738,\n",
              " 739: 739,\n",
              " 740: 740,\n",
              " 741: 741,\n",
              " 742: 742,\n",
              " 743: 743,\n",
              " 744: 744,\n",
              " 745: 745,\n",
              " 746: 746,\n",
              " 747: 747,\n",
              " 748: 748,\n",
              " 749: 749,\n",
              " 750: 750,\n",
              " 751: 751,\n",
              " 752: 752,\n",
              " 753: 753,\n",
              " 754: 754,\n",
              " 755: 755,\n",
              " 756: 756,\n",
              " 757: 757,\n",
              " 758: 758,\n",
              " 759: 759,\n",
              " 760: 760,\n",
              " 761: 761,\n",
              " 762: 762,\n",
              " 763: 763,\n",
              " 764: 764,\n",
              " 765: 765,\n",
              " 766: 766,\n",
              " 767: 767,\n",
              " 768: 768,\n",
              " 769: 769,\n",
              " 770: 770,\n",
              " 771: 771,\n",
              " 772: 772,\n",
              " 773: 773,\n",
              " 774: 774,\n",
              " 775: 775,\n",
              " 776: 776,\n",
              " 777: 777,\n",
              " 778: 778,\n",
              " 779: 779,\n",
              " 780: 780,\n",
              " 781: 781,\n",
              " 782: 782,\n",
              " 783: 783,\n",
              " 784: 784,\n",
              " 785: 785,\n",
              " 786: 786,\n",
              " 787: 787,\n",
              " 788: 788,\n",
              " 789: 789,\n",
              " 790: 790,\n",
              " 791: 791,\n",
              " 792: 792,\n",
              " 793: 793,\n",
              " 794: 794,\n",
              " 795: 795,\n",
              " 796: 796,\n",
              " 797: 797,\n",
              " 798: 798,\n",
              " 799: 799,\n",
              " 800: 800,\n",
              " 801: 801,\n",
              " 802: 802,\n",
              " 803: 803,\n",
              " 804: 804,\n",
              " 805: 805,\n",
              " 806: 806,\n",
              " 807: 807,\n",
              " 808: 808,\n",
              " 809: 809,\n",
              " 810: 810,\n",
              " 811: 811,\n",
              " 812: 812,\n",
              " 813: 813,\n",
              " 814: 814,\n",
              " 815: 815,\n",
              " 816: 816,\n",
              " 817: 817,\n",
              " 818: 818,\n",
              " 819: 819,\n",
              " 820: 820,\n",
              " 821: 821,\n",
              " 822: 822,\n",
              " 823: 823,\n",
              " 824: 824,\n",
              " 825: 825,\n",
              " 826: 826,\n",
              " 827: 827,\n",
              " 828: 828,\n",
              " 829: 829,\n",
              " 830: 830,\n",
              " 831: 831,\n",
              " 832: 832,\n",
              " 833: 833,\n",
              " 834: 834,\n",
              " 835: 835,\n",
              " 836: 836,\n",
              " 837: 837,\n",
              " 838: 838,\n",
              " 839: 839,\n",
              " 840: 840,\n",
              " 841: 841,\n",
              " 842: 842,\n",
              " 843: 843,\n",
              " 844: 844,\n",
              " 845: 845,\n",
              " 846: 846,\n",
              " 847: 847,\n",
              " 848: 848,\n",
              " 849: 849,\n",
              " 850: 850,\n",
              " 851: 851,\n",
              " 852: 852,\n",
              " 853: 853,\n",
              " 854: 854,\n",
              " 855: 855,\n",
              " 856: 856,\n",
              " 857: 857,\n",
              " 858: 858,\n",
              " 859: 859,\n",
              " 860: 860,\n",
              " 861: 861,\n",
              " 862: 862,\n",
              " 863: 863,\n",
              " 864: 864,\n",
              " 865: 865,\n",
              " 866: 866,\n",
              " 867: 867,\n",
              " 868: 868,\n",
              " 869: 869,\n",
              " 870: 870,\n",
              " 871: 871,\n",
              " 872: 872,\n",
              " 873: 873,\n",
              " 874: 874,\n",
              " 875: 875,\n",
              " 876: 876,\n",
              " 877: 877,\n",
              " 878: 878,\n",
              " 879: 879,\n",
              " 880: 880,\n",
              " 881: 881,\n",
              " 882: 882,\n",
              " 883: 883,\n",
              " 884: 884,\n",
              " 885: 885,\n",
              " 886: 886,\n",
              " 887: 887,\n",
              " 888: 888,\n",
              " 889: 889,\n",
              " 890: 890,\n",
              " 891: 891,\n",
              " 892: 892,\n",
              " 893: 893,\n",
              " 894: 894,\n",
              " 895: 895,\n",
              " 896: 896,\n",
              " 897: 897,\n",
              " 898: 898,\n",
              " 899: 899,\n",
              " 900: 900,\n",
              " 901: 901,\n",
              " 902: 902,\n",
              " 903: 903,\n",
              " 904: 904,\n",
              " 905: 905,\n",
              " 906: 906,\n",
              " 907: 907,\n",
              " 908: 908,\n",
              " 909: 909,\n",
              " 910: 910,\n",
              " 911: 911,\n",
              " 912: 912,\n",
              " 913: 913,\n",
              " 914: 914,\n",
              " 915: 915,\n",
              " 916: 916,\n",
              " 917: 917,\n",
              " 918: 918,\n",
              " 919: 919,\n",
              " 920: 920,\n",
              " 921: 921,\n",
              " 922: 922,\n",
              " 923: 923,\n",
              " 924: 924,\n",
              " 925: 925,\n",
              " 926: 926,\n",
              " 927: 927,\n",
              " 928: 928,\n",
              " 929: 929,\n",
              " 930: 930,\n",
              " 931: 931,\n",
              " 932: 932,\n",
              " 933: 933,\n",
              " 934: 934,\n",
              " 935: 935,\n",
              " 936: 936,\n",
              " 937: 937,\n",
              " 938: 938,\n",
              " 939: 939,\n",
              " 940: 940,\n",
              " 941: 941,\n",
              " 942: 942,\n",
              " 943: 943,\n",
              " 944: 944,\n",
              " 945: 945,\n",
              " 946: 946,\n",
              " 947: 947,\n",
              " 948: 948,\n",
              " 949: 949,\n",
              " 950: 950,\n",
              " 951: 951,\n",
              " 952: 952,\n",
              " 953: 953,\n",
              " 954: 954,\n",
              " 955: 955,\n",
              " 956: 956,\n",
              " 957: 957,\n",
              " 958: 958,\n",
              " 959: 959,\n",
              " 960: 960,\n",
              " 961: 961,\n",
              " 962: 962,\n",
              " 963: 963,\n",
              " 964: 964,\n",
              " 965: 965,\n",
              " 966: 966,\n",
              " 967: 967,\n",
              " 968: 968,\n",
              " 969: 969,\n",
              " 970: 970,\n",
              " 971: 971,\n",
              " 972: 972,\n",
              " 973: 973,\n",
              " 974: 974,\n",
              " 975: 975,\n",
              " 976: 976,\n",
              " 977: 977,\n",
              " 978: 978,\n",
              " 979: 979,\n",
              " 980: 980,\n",
              " 981: 981,\n",
              " 982: 982,\n",
              " 983: 983,\n",
              " 984: 984,\n",
              " 985: 985,\n",
              " 986: 986,\n",
              " 987: 987,\n",
              " 988: 988,\n",
              " 989: 989,\n",
              " 990: 990,\n",
              " 991: 991,\n",
              " 992: 992,\n",
              " 993: 993,\n",
              " 994: 994,\n",
              " 995: 995,\n",
              " 996: 996,\n",
              " 997: 997,\n",
              " 998: 998,\n",
              " 999: 999}"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BauN4rPzH38U"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # # Populate the first character of target sequence with the start character.\n",
        "    # target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = [ ]\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence.append(sampled_char)\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbPwmkrYCfUT"
      },
      "source": [
        "You can now generate decoded sentences as such:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU3pUrZXIGFc",
        "outputId": "cddffe50-822d-48db-b39c-cfee84309cfb"
      },
      "source": [
        "inwave[1]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[50,\n",
              " 51,\n",
              " 52,\n",
              " 53,\n",
              " 54,\n",
              " 55,\n",
              " 56,\n",
              " 57,\n",
              " 58,\n",
              " 59,\n",
              " 60,\n",
              " 61,\n",
              " 62,\n",
              " 63,\n",
              " 64,\n",
              " 65,\n",
              " 66,\n",
              " 67,\n",
              " 68,\n",
              " 69,\n",
              " 70,\n",
              " 71,\n",
              " 72,\n",
              " 73,\n",
              " 74,\n",
              " 75,\n",
              " 76,\n",
              " 77,\n",
              " 78,\n",
              " 79,\n",
              " 80,\n",
              " 81,\n",
              " 82,\n",
              " 83,\n",
              " 84,\n",
              " 85,\n",
              " 86,\n",
              " 87,\n",
              " 88,\n",
              " 89,\n",
              " 90,\n",
              " 91,\n",
              " 92,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 96,\n",
              " 97,\n",
              " 98,\n",
              " 99]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXFi9TBRILa6",
        "outputId": "34452f2a-9ef5-4629-eed0-645891ac7c3c"
      },
      "source": [
        "tarwave[1]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[950,\n",
              " 949,\n",
              " 948,\n",
              " 947,\n",
              " 946,\n",
              " 945,\n",
              " 944,\n",
              " 943,\n",
              " 942,\n",
              " 941,\n",
              " 940,\n",
              " 939,\n",
              " 938,\n",
              " 937,\n",
              " 936,\n",
              " 935,\n",
              " 934,\n",
              " 933,\n",
              " 932,\n",
              " 931,\n",
              " 930,\n",
              " 929,\n",
              " 928,\n",
              " 927,\n",
              " 926,\n",
              " 925,\n",
              " 924,\n",
              " 923,\n",
              " 922,\n",
              " 921,\n",
              " 920,\n",
              " 919,\n",
              " 918,\n",
              " 917,\n",
              " 916,\n",
              " 915,\n",
              " 914,\n",
              " 913,\n",
              " 912,\n",
              " 911,\n",
              " 910,\n",
              " 909,\n",
              " 908,\n",
              " 907,\n",
              " 906,\n",
              " 905,\n",
              " 904,\n",
              " 903,\n",
              " 902,\n",
              " 901]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcVG0RJ8CfUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f29d8b73-5720-4952-dbe8-9a5178604b86"
      },
      "source": [
        "for seq_index in range(5):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print(\"-\")\n",
        "    print(\"Input sentence:\", inwave[seq_index])\n",
        "    print(\"Decoded sentence:\", decoded_sentence)\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Input sentence: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Decoded sentence: [999, 998, 997, 996, 995, 994, 993, 992, 991, 990, 989, 988, 987, 986, 985, 984, 983, 982, 981, 980, 979, 978, 977, 976, 975, 974, 973, 972, 971, 970, 969, 968, 967, 966, 965, 964, 963, 962, 961, 960, 959, 958, 957, 956, 955, 954, 953, 952, 951, 1, 1]\n",
            "-\n",
            "Input sentence: [50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Decoded sentence: [949, 948, 947, 946, 945, 944, 943, 942, 941, 940, 939, 938, 937, 936, 935, 934, 933, 932, 931, 930, 929, 928, 927, 926, 925, 924, 923, 922, 921, 920, 919, 918, 917, 916, 915, 914, 913, 912, 911, 910, 909, 908, 907, 906, 905, 904, 903, 902, 901, 1, 1]\n",
            "-\n",
            "Input sentence: [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149]\n",
            "Decoded sentence: [899, 898, 897, 896, 895, 894, 893, 892, 891, 890, 889, 888, 887, 886, 885, 884, 883, 882, 881, 880, 879, 878, 877, 876, 875, 874, 873, 872, 871, 870, 869, 868, 867, 866, 865, 864, 863, 862, 861, 860, 859, 858, 857, 856, 855, 854, 853, 852, 851, 1, 1]\n",
            "-\n",
            "Input sentence: [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]\n",
            "Decoded sentence: [849, 848, 847, 846, 845, 844, 843, 842, 841, 840, 839, 838, 837, 836, 835, 834, 833, 832, 831, 830, 829, 828, 827, 826, 825, 824, 823, 822, 821, 820, 819, 818, 817, 816, 815, 814, 813, 812, 811, 810, 809, 808, 807, 806, 805, 804, 803, 802, 801, 1, 1]\n",
            "-\n",
            "Input sentence: [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249]\n",
            "Decoded sentence: [799, 798, 797, 796, 795, 794, 793, 792, 791, 790, 789, 788, 787, 786, 785, 784, 783, 782, 781, 780, 779, 778, 777, 776, 775, 774, 773, 772, 771, 770, 769, 768, 767, 766, 765, 764, 763, 762, 761, 760, 759, 758, 757, 756, 755, 754, 753, 752, 751, 1, 1]\n"
          ]
        }
      ]
    }
  ]
}